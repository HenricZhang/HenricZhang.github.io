、第一章 课程技术总结

完整项目：通讯框架+业务逻辑框架

## 1、 项目内容

（1）项目是非常完整的多线程高并发服务器程序

（2）按照包头+包体格式收，完美地解决了数据粘包的问题

（3）根据收到的数据包来执行不同的业务逻辑

（4）把业务处理产生的结果数据包正确的返回给客户端

## 2、 用到的主要开发技术

（1）epoll高并发通讯技术，用的是水平触发【LT】，简单提及边缘触发模式【ET】

（2）通过线程池技术处理业务逻辑

（3）多线程，线程之间的同步技术包括互斥量、信号量等等

（4）次要技术：信号、日志打印，fork()创建子进程，守护进程怎么写

## 3、 借鉴了官方nginx哪些精华代码

（1）一个master进程，多个work进程的进程框架

（2）epoll实现代码，但官方epoll触发模式用的是ET【我们项目用的是LT】

（3）借鉴了官方nginx的接收数据包以及发送数据包的核心代码

## 4、 哪些是我们没有借鉴官方nginx而独立实现的代码

（1）epoll LT模式

（2）自己写了一套线程池来处理业务逻辑，调用适当的业务逻辑来处理函数，直到处理完毕把数据发送回客户端

（3）连接池中的连接的延迟回收（**精华技术**）

（4）专门处理数据发送的一整套数据发送逻辑以及发送线程

# 第二章 开发基础知识

## 1、终端与进程的关系

（1）、pts(虚拟终端)：xshell每连接一个窗口到虚拟机，就出现一个bash进程（黑窗口），用来解释用户输入。

（2）、终端上开启进程

​      用./nginx启动nginx，就可以知道bash是nginx的父进程，所以bash(终端)退出了，进程也退出了。

（3）、进程关系进一步分析

​    进程组：一个或多个进程的集合，每个进程组有唯一的ID，由系统函数来创建和加入组会话（session）：一个或多个进程组的集合。

<span style ='color:red;background:背景颜色;font-size:文字大小;'>一般来说，一个bash上的所有进程都属于一个会话，这个bash进程就是session leader</span>

若断开xshell终端，系统会向session leader发送SIGHUP信号，session leader会将信号发给session里所有进程，最后再发送给自己，这也解释了为什么关闭终端，nginx进程也停止了。

（4）、strace工具：跟踪程序执行时系统调用以及收到的信号（附着）

```shell
sudo strace -e trace=signal -p 进程号
```

（5）、终端关闭时如何让进程不退出

方法一：nginx进程拦截SIGHUP信号，告诉OS不要退出

a）nohup ./nginx启动nginx，忽略SIGHUP信号，而且屏幕输出重定位到当前的nohup.out中；

b）代码中加入如下内容以忽略SIGHUP信号。

```c++
signal(SIGHUP,SIG_IGN)；
```

此时关掉终端，父死子活，nginx的PPID为1，TT为?,变为孤儿进程!

方法二：nginx进程和bash进程不在同一个session（其实就是创建了一个守护进程）

a）直接setsid ./nginx 启动nginx；

b）代码中如下:

```c++
#include <stdio.h>
#include <unistd.h>
int main(int argc, char* const* argv)
{
	pid_t pid;
	pid = fork();
	if(pid < 0)
	{
		printf("fork()进程出错! \n");
	}
	else if(pid == 0)
	{
		printf("子进程开始执行! \n");
		setsid();  //新建立一个不同的session, 但是进程组组长调用setsid()是无效的
		fork(;;)
		{
			sleep(1); //休息1秒
			printf("子进程休息1秒 \n");
		}
		return 0;
	}
	else
	{
		//父进程会走到这里
		for(;;)
		{
				sleep(1); //休息1秒
				printf("父进程休息1秒 \n");
		}
		return 0;
	}
	return 0;
}
```

如下图可知，该nginx进程不隶属于任何终端，SID也与bash的不同，所以关闭bash不会往此进程发SIGHUP信号。

![](E:\jianguoyun\我的坚果云\nginx\20190401164746640.png)

（6）、后台运行：在后面加&, ./nginx &

后台运行正常操作，如ls，cd等都可以显示信息，所以ctrl+c也是不能终止进程的，只能fg切换到前台在ctrl+c，当然前台ls，cd等命令是没有效果的。

**关闭终端，进程停止，这不取决于进程在前台还是后台运行。**

## 2、信号

（1）、基本概念

进程间常用的通信手段，如kill掉一个worker进程，master进程就会立即启动一个新的worker进程，**信号用来通知某一个进程发生了某个事情（突发事情），所以进程不知道什么时候收到信号，也就是说信号是异步发生的，也被称为软件中断。**

【信号如何产生】

a）某个进程发给另一个进程或发给自己

b）由内核（操作系统）发送给某个进程（ctrl + c 或 kill 或 内存访问异常 或 除以0）

信号名字：以SIG开头，如SIGHUP（终端断开信号），也就是一些数字（正整数常量，系统头文件中的宏）。

（2）、kill

**kill其实是给进程发信号，能发多种信号，而不只是杀死进程的意思！**

单纯的kill，其实是给进程发送了SIGTERM信号（终止信号），也就是kill -15 PID。

【常用数字】（很多信号的缺省动作都是杀掉进程）

1 SIGHUP

2 SIGINT（类似ctrl + c）

9 无视代码，直接kill

18 SIGCONT 使暂停的进程继续（运行在后台）

19 SIGSTOP 停止进程但后台还在

20 SIGSTP 终端停止但后台还在（类似ctrl + z）

查看进程状态

```shell
ps -aux | grep -E 'bash|PID|nginx'
```

![](E:\jianguoyun\我的坚果云\nginx\20190401173415376.png)

（3）、某个信号出现时，3种方式处理

a）执行系统默认动作（绝大多数信号是杀进程）；

b）忽略此信号（无法忽略SIGKILL和SIGSTOP，也就是-9和-19）；

c）捕捉该信号（加入处理信号的函数）

## 3、Linux体系结构与信号编程初步

（1）、Unix/Linux操作系统体系结构

类Unix操作系统体系结构分为2个状态：用户态和内核态

![](E:\jianguoyun\我的坚果云\nginx\2019040121174676.png)

![](E:\jianguoyun\我的坚果云\nginx\20190401211815127.png)

a）os（内核）：控制硬件资源，提供应用程序运行环境

写的程序不是运行在用户态就是内核态，一般在用户态；当程序要执行特殊代码时，会自动切换到内核态（无需人为介入）。

b）系统调用：就是一些系统函数

c）shell：bash（borne again shell）是shell的一种，linux上默认采用bash这种shell，bash是一个可执行程序，充当命令解释器的作用，也就是把用户输入的命令翻译给os。

whereis bash ， /bin/bash可以在bash里运行一个bash，exit可以退出当前bash！

d）用户态和内核态的切换（根据需要自动切换）

用户态权限小，内核态权限大！

【为什么区分？】

i）一般情况下运行在用户态，权限小，不至于危害到其他部分；

ii）危害部分操作系统会进行统一管理，系统提供的这些接口就是为了减少有限资源的范围和使用上的冲突。

【什么时候切换到内核态？】

i）系统调用：如malloc()、printf()；

ii）异常事件：比如来了个信号，在内核态中调用信号处理函数；

iii）外围设备中断：导致程序处理流程从用户态跳到内核态

（2）、signal函数范例（捕捉信号）

```c++
if(signal(SIGUSR1,sig_usr) == SIG_ERR)  
//系统函数，参数1：是个信号，参数2：是个函数指针，代表一个针对该信号的捕捉处理函数
{
    printf("无法捕捉SIGUSR1信号!\n");
}
```

这样当kill -USR1 pid时，该进程会调用sig_usr这个信号处理函数：

```c++
void sig_usr(int signo)
{     
    if(signo == SIGUSR1)
    {
        printf("收到了SIGUSR1信号!\n");
    }
    else
    {
        printf("收到了未捕捉的信号%d!\n",signo);
    }
}
```

**进程收到信号，会被内核注意到，具体流程如下：**

![](E:\jianguoyun\我的坚果云\nginx\20190401212848671.png)

【问题】如果有一个全局变量，在main中和在信号处理函数中都调用，此时来了信号，先去执行了信号处理函数而改变了此值，就会影响该值在main的计算结果？

**可重入函数：所谓的可重入函数，就是我们在信号处理函数中调用它是安全的。**

**不可重入函数如：malloc、printf、给全局变量赋值的函数等。**

在写信号处理函数的时候，要注意的事项：

a）在信号处理函数中，尽量使用简单的语句做简单的事情，尽量不要调用系统函数以免引起麻烦；

b）如果必须要在信号处理函数中调用一些系统函数，那么要保证在信号处理函数中调用的系统函数一定要是可重入的（有个表）；

c）如果必须要在信号处理函数中调用那些可能修改errno值（出现一些错误系统的返回值）的可重入的系统函数，那么就得事先备份errno值，从信号处理函数返回之前，将errno值恢复。

**信号处理函数中一定一定一定要用可重入函数！**

**signal因为兼容性和可靠性等一些历史问题，不建议使用，用sigaction()函数代替！**

## 4、信号编程进阶、sigprocmask

（1）信号集

【问题】当一个信号处理函数运行时，系统会屏蔽此段时间内的其他的信号，但必须记住，排队！

信号集：装60多种信号来或者没来的状态，1表示来了，0表示没来。

00000000，00000000，00000000，00000000，00000000，00000000，00000000，00000000（64个二进制位）

linux中用sigset_t结构类型来表示

（2）信号相关函数（一个进程对应一个信号集）

a）sigemptyset()：把信号集中的所有信号都清0，表示这60多个信号都没有来，000000000000000000...

b）sigfillset()：把信号集中的所有信号都设置为1，跟sigemptyset()正好相关，1111111111111111111...

c）用sigaddset()和sigdelset()就可以往信号集中增加信号，或者从信号集中删除特定信号；

d）sigprocmask、sigmember

**一个进程，里边会有一个信号集，用来记录当前屏蔽（阻塞）了哪些信号，sigprocmask就是用来绑定某个信号集与进程的。**

如果我们把这个信号集中的某个信号位置位1，就表示屏蔽了同类信号，此时再来个同类信号，那么同类信号会被屏蔽，不能传递给进程；如果这个信号集中有很多个信号位都被设置为1，那么所有这些被设置为1的信号都是属于当前被阻塞的而不能传递到该进程的信号。

**sigprocmask()函数就能够设置该进程所对应的信号集中的内容，注意要先注册信号处理函数再设置相关的信号集。**

demo关键代码:

```c++
sigset_t newmask,oldmask; //信号集，新的信号集，原有的信号集
if(signal(SIGQUIT,sig_quit) == SIG_ERR)  //注册信号对应的信号处理函数,"ctrl+\" 
{        
    printf("无法捕捉SIGQUIT信号!\n");
    exit(1);
}
//newmask信号集中所有信号都清0（表示这些信号都没有来）
sigemptyset(&newmask);
//设置newmask信号集中的SIGQUIT信号位为1，再来SIGQUIT信号时，进程就收不到
sigaddset(&newmask,SIGQUIT); 
//sigprocmask()：设置该进程所对应的信号集
//第一个参数和第二个参数取并集作为当前进程的信号集，因为SIG_BLOCK为全0，
//所以其实就是用newmask作为当前进程的信号集，
//第三个参数用来保存之前的信号集，故oldmask为全0
if(sigprocmask(SIG_BLOCK,&newmask,&oldmask) < 0)
{
    printf("sigprocmask(SIG_BLOCK)失败!\n");
    exit(1);
}
//测试一个指定的信号位是否被置位(为1)，测试的是newmask的SIGQUIT位，此处应该是屏蔽了
if(sigismember(&newmask,SIGQUIT))
{
    printf("SIGQUIT信号被屏蔽了!\n");
}
...
//第一个参数用了SIGSETMASK表明设置进程新的信号屏蔽字为第二个参数指向的信号集，第三个参数没用
if(sigprocmask(SIG_SETMASK,&oldmask,NULL) < 0) 
{
    printf("sigprocmask(SIG_SETMASK)失败!\n");
    exit(1);
}
```

如果在屏蔽期间发了数个"ctrl + \"信号，则在打开屏蔽后进程会合n为1，只收到一个"ctrl + \"信号。

**sleep()函数能够被打断，来了某个信号会使sleep()提前结束，sleep会返回一个值，这个值就是未睡够的时间！**

**如果在信号处理函数中加入这几行，则第二次信号到来时，会默认为缺省处理（终止进程），直接quit终止进程（不"再见"）！**

```c++
if(signal(SIGQUIT,SIG_DFL) == SIG_ERR)
{
    printf("无法为SIGQUIT信号设置缺省处理(终止进程)!\n");
    exit(1);
}
```

**以后（商业代码）sigaction要取代signal！**

## 5、fork

（1）fork函数简单认识

进程：一个可执行程序，执行起来就是一个进程，再执行起来一次又是一个进程（多个进程可以共享同一个可执行文件）。

**其他解释：程序执行的一个实例，用fork创建一个子进程，相当于创建含有相同一段的两条执行通路。**

图示如下：

![](E:\jianguoyun\我的坚果云\nginx\20190404090619917.png)

**fork()之后，是父进程fork()之后的代码先执行还是子进程fork()之后的代码先执行是不一定的，这个跟内核调度算法有关！**

**【问题】kill子进程，观察父进程收到什么信号？**

**【回答】用strace，父进程收到来自子进程的SIGCHLD信号，子进程随后变为僵尸进程，STAT为Z+。僵尸进程占用资源的，至少占用pid号，系统中是有限制的，所以开发者要杜绝僵尸进程的存在！**

（2）、僵尸进程的产生和解决

1）产生

在Unix系统中，**子死（可能是被kill也可能只是结束了）父活**，但父没有调用（wait/waitpid）函数来进行额外的处置，子进程就会变成一个僵尸进程；

**僵尸进程已经终止，不干活了，但还没有被内核丢弃，因为内核认为父进程可能还用子进程的一些信息。**

2）解决

a）重启电脑；

b）手工地把僵尸进程的父进程kill掉，僵尸进程就会自动消失；

**c）SIGCHLD信号：一个进程被终止或者停止时，这个信号会被子进程发送给父进程；所以，对于源码中有fork()行为的进程，我们应该拦截并处理SIGCHLD信号。**

```C++
pid_t pid = waitpid(-1,&status,WNOHANG);
//第一个参数为-1，表示等待任何子进程
//第二个参数：保存子进程的状态信息
//第三个参数：提供额外选项，WNOHANG表示不要阻塞，让这个waitpid()立即返回
if(pid == 0)
//子进程没结束，会立即返回这个数字，但这里应该不是这个数字                        
    return;
if(pid == -1)
//这表示这个waitpid调用有错误，有错误也立即返回
    return;
//走到这里，表示成功，直接return
return;   
```

（3）、fork函数的进一步认识**（写时复制机制）**

fork()产生新进程的**速度非常快**，fork()产生的新进程并不复制原进程的内存空间，而是和原进程（父进程）一起共享一个内存空间，但这个内存空间的特性是**"写时复制"**，也就是说：原来的进程和fork()出来的子进程可以同时、自由的读取内存，但如果子进程（父进程）对内存进行修改的话，那么这个内存就会复制一份给其他进程单独使用，以免影响到共享这个内存空间的其他进程使用。

（4）、完善fork()代码

**fork()会返回两次：**父进程中返回一次，子进程中返回一次。而且，fork()在父进程中返回的值和在子进程中返回的值是不同的，子进程的fork()的返回值是0，父进程的fork()返回值是新建立的子进程的ID（所以返回pid是大于0的）。

如果有一个全局变量g_mygbltest，而且某个进程中对该值有改变动作，会导致父子进程内存分开（写时复制机制），所以即使是全局变量，两个g_mygbltest的值也是不同的（因为是在不同的进程中）。

（5）一道逻辑题

**连续fork()两次，创建4个进程；如下操作，产生7个进程，注意短路求值！**

```c++
((fork() && fork()) || (fork() && fork()));
```

（6）fork失败的可能性

也就是超过这些量fork进程会失败！

a）系统中进程太多：缺省情况最大的pid为32767;

b）每个用户有个允许开启的进程总数：sysconf（_SC_CHILD_MAX）查看，大约7788。

## 6、守护进程

（1）回顾

```shell
ps -eo pid,ppid,sid,tty,pgrp,comm,stat,cmd | grep -E 'bash|PID|nginx'
```

a）进程有对应的终端，如果终端退出，那么对应的进程也就消失了，它的父进程是一个bash；

b）终端被占住了，输入各种命令这个终端都没有反应。

（2）基本概念

**一种长期运行的进程，这种进程在后台运行，并且不跟任何的控制终端关联。**

【基本特点】

a）生存期长（不是必须，但一般这样做），一般是操作系统启动的时候他就启动，操作系统关闭的时候它才关闭；

b）守护进程跟终端无关联，也就是说他们没有控制终端，所以你控制终端退出，也不会导致守护进程退出；

c）守护进程是在后台运行，不会占着终端，终端可以执行其他命令。

linux操作系统本身是有很多守护进程在默默地运行，维持着系统的日常活动。大概30~50个。

```shell
ps -efj    //e所有进程，f完整格式，j与作业或任务相关
```

a）ppid = 0：内核进程，跟随系统启动而启动，生命周期贯穿整个系统；

b）CMD列名字带[ ]这种，叫内核守护进程；

c）老祖init：也是守护进程，它负责启动各运行层次特定的系统服务；所以很多进程的PPID是init，而且**这个init也负责收养孤儿进程；**

d）CMD列中名字不带[ ]的普通守护进程（用户级守护进程）

【共同点总结】

a）大多数守护进程都是以超级用户特权运行的

b）守护进程没有控制终端，TT这列显示？

c）内核守护进程以无控制终端方式启动

d）普通守护进程是守护进程调用了setsid的结果（无控制端）

（3）守护进程编写规则

a）调用umask(0)：umask是个函数，用来限制（屏蔽）一些文件权限的。

b）fork()一个子进程（脱离终端）出来，然后父进程退出（把终端空出来，不让终端卡住），固定套路。

fork()的目的是想成功调用setsid()来建立新会话，目的是子进程有单独的sid（因为进程组组长没法setsid），这样，子进程成为了一个新进程组的组长进程，子进程也不关联任何终端了。

（4）其他重要概念

a）文件描述符：正数，用来标识一个文件。

当你打开一个存在的文件或者创建一个新文件，操作系统都会返回这个文件描述符（其实就是代表这个文件的），后续对这个文件的操作的一些函数，都会用到这个文件描述符作为参数；

linux中的三个特殊的文件描述符，数字分别为0，1，2

0：标准输入【键盘】，对应的符号常量叫STDIN_FILENO

1：标准输出【屏幕】，对应的符号常量叫STDOUT_FILENO

2：标准错误【屏幕】，对应的符号常量叫STDERR_FILENO

类Unix操作系统，默认从STDIN_FILENO读数据，向STDOUT_FILENO来写数据，向STDERR_FILENO来写错误。一切皆文件，把标准输入，标准输出，标准错误都看成文件。同时，程序一旦运行起来，这三个文件描述符0，1，2会被自动打开（自动指向对应的设备）。

文件描述符虽然是数字，但如果我们把文件描述符直接理解成指针（指针里边保存的是地址---地址说白了也是个数字）。

```c++
write(STDOUT_FILENO,"aaaabbb",6);    //屏幕上输出aaaabb
```

b）输入输出重定向

输出重定向：标准输出文件描述符，不指向屏幕了，加入我指向（重定向）一个文件；

输出重定向在命令行中用 > 可将本来显示在屏幕上的内容放入myoutfile文件：

```shell
ls -la > myoutfile
```

输入重定向，相当于输入的myinfile的内容：

```shell
cat < myinfile
```

合用，把myinfile里的内容当作输入通过cat输出，但是将本来显示在屏幕上的内容放入myoutfile：

```shell
cat < myinfile > myoutfile
```

c）空设备

**/dev/null：**是一个特殊的设备文件，它丢弃一切写入其中的数据（像黑洞一样）。

【注意】守护进程虽然可以通过终端启动，但是和终端不挂钩。守护进程是在后台运行，它不应该从键盘上接收任何东西，也不应该把输出结果打印到屏幕或者终端上来。所以，一般按照江湖规矩，我们**把守护进程的标准输入和标准输出重定向到空设备**（黑洞）。从而确保守护进程不从键盘接收任何东西，也不把输出结果打印到屏幕。

【核心代码demo】

```c++
int fd;
fd = open("/dev/null",O_RDWR);    //打开空设备
dup2(fd,STDIN_FILENO);    //复制文件描述符，像个指针赋值，把第一个参数指向的内容赋给了第二个参数
dup2(fd,STDOUT_FILENO);    //同上
if(fd > STDERR_FILENO)    //012都被占，fd至少是个3
    close(fd);    //等价于fd = null;
```

dup2图示如下：**（dup2还可以先关闭原来指向的文件描述符，所以商业代码中尽量用dup2）** 

![](E:\jianguoyun\我的坚果云\nginx\20190404164327832.png)

**守护进程可以用命令启动，如果想开机启动，则需要借助系统初始化脚本来启动。**

（5）守住进程不会收到的信号

a）SIGHUP信号：守护进程不会收到来自内核的 SIGHUP 信号，潜台词就是如果守护进程收到了 SIGHUP 信号，那么肯定是另外的进程给发来的（SIGHUP是Session Leader发给其他进程的，守护进程不关联终端，所以不会收到）。

很多守护进程把这个信号作为通知信号，表示配置文件已经发生改动，守护进程应该重新读入其配置文件。

如在nginx中，就是用SIGHUP信号来通知会话首进程（master）配置文件有变动，需要重启4个worker进程。

```shell
sudo ./nginx -s reload    //执行这行后，重启4个worker进程
等价于
sudo kill -1 master进程号
```

b）SIGINT、SIGWINCH信号

守护进程不会收到来自内核的 SIGINT（ctrl+c)，SIGWINCH（终端窗口大小改变）信号，所以可以拿来自己用。

（6）守护进程和后台进程的区别

a）守护进程和终端不挂钩，后台进程能往终端上输出东西（如 printf 照样打印，是和终端挂钩的）；

b）守护进程在关闭终端时不受影响，后台进程会随着终端的退出而退出。

# 第三章 服务器程序框架

## 1、服务器程序目录规划、makefile编写

（1）信号高级认识范例

在触发SIGUSR1信号并因此sleep的10秒钟期间，就算你多次触发SIGUSR1信号，也不会重新执行SIGUSR1信号对应的信号处理函数，而是会等这个信号处理程序执行完了，把那些信号归为一次执行一次信号处理程序。但是，如果在USR1的信号处理程序执行时收到了USR2，则会跳到USR2的信号处理程序，如果不想让它跳，应屏蔽该信号，后续有讲解！

（2）目录结构规划（make编译）

主目录名nginx：

_include：专门存放各种头文件；

app目录：放主应用程序.c（main()函数所在的文件）以及一些比较核心的文件；

net目录：专门存放和网络处理相关的1到多个.c文件；

proc目录：专门存放和进程处理有关1到多个.c文件；

signal目录：专门用于存放和信号处理有关的1到多个.c文件。

**linux上用tree可以查看目录结构**

（3）编译工具make的使用概述（编译出可执行文件）

vs2017可能可以编译linux下项目，但不确定，所以用传统的，经过验证没有问题的方式来编译项目，最终生成可执行文件。

每个.c生成一个.0，多个.c生成多个.o，最终这些.o被链接到一起，生成一个可执行文件，如下图：

![](E:\jianguoyun\我的坚果云\nginx\20190407132050462.png)

正常来讲，编译文件需要如下方式，但如果项目很大且有很多文件夹时显然不现实。

```shell
gcc -o nginx ng1.c
gcc -o nginx ng1.c ng2.c
```

**所以要借助make的命令来编译，最终生成可执行文件，大型项目一般都用make。**

make命令的工作原理：去当前目录读取一个叫做makefile的文件（文本文件），根据这个makefile文件里的规则把源代码编译成可执行文件，开发者的任务就是把这个makefile文件写出来。

**makefile文件：文本文件，utf-8编码格式，没有扩展名，一般放在根目录下【也会根据需要放在子目录】。**

makefile里边定义了怎么去编译整个项目的编译、链接规则，实际上makefile文件就是一个我们编译工程要用到的各种源文件等等的一个依赖关系描述，也有类似autotools自动生成makefile。

【makefile文件的编写】

**第一步：**nginx根目录下放三个文件：

1）makefile：编译项目的入口脚本，编译项目从这里开始，起总体控制作用；

2）config.mk：配置脚本，被makefile文件include，单独分离出来是为了应付一些可变的东西，一般变动的东西都往这里；

3）common.mk：最重要最核心的编译脚本，定义makefile的编译规则，依赖规则等，各个子目录中都用到这个脚本来实现对应子目录的.c文件的编译。

**第二步：**每个子目录下都有一个makefile文件，每个makefile文件都会包括根目录下的common.mk从而实现自己这个子目录下的.c文件的编译，限制的makefile不支持目录中套子目录。子目录下makefile文件如下：

```makefile
BIN =     //子目录中只生成.d和.o文件，不要求生成可执行文件，所以为空！
include $(BUILD_ROOT)/common.mk
```

【注意】app/link_obj临时目录，存放.o目标文件，app/dep：存放.d开头的依赖关系文件，make之后在app目录下自动生成。

（4）makefile脚本用法介绍

**直接在根目录下make，会在app目录中生成两个中间文件夹，和根目录下的一个可执行文件nginx。**

make后若想删除这两个多余的文件夹可以make clean，这其实是执行了根目录下makefile文件中的如下两行：

```makefile
clean:
#-rf：删除文件夹，强制删除
	rm -rf app/link_obj app/dep nginx
	rm -rf signal/*.gch app/*.gch
```

make原理大致如下：

![](E:\jianguoyun\我的坚果云\nginx\20190407132050462.png)

（5）makefile脚本具体实现讲解

从common.mk（因为所有子目录的makefile也用到了common.mk），SRCS扫描所有目录下.c文件!

```shell
all:$(DEPS) $(OBJS) $(BIN)    //以冒号分界：左侧为目标，右侧为依赖
```

**makefile流程：在根目录下make，先找到makefile文件，for循环遍历所有文件加进行make（这里include了config.mk），这时里面的make会include common.mk文件并利用其规则生成依赖，并连接每个.o文件生成bin。**

将来增加新目录时:

a）修改根目录下的config.mk来增加该目录，注意app应在最后；

b）在对应的目录下放入makefile文件，内容参考signal目录下的makefile文件即可。

## 2、读配置文件、查泄露、设置标题实战

（1）基础设施之配置文件读取

使用配置文件，使服务器程序有了极大的灵活性，作为服务器程序开发者，必须首先搞定这些问题。

配置文件：文本文件，里边除了注释行之外（以#号开头的行）不要用中文，只在配置文件中使用字母、数字、下划线。

写代码要多顾及别人感受，让别人更容易读懂和理解，该缩进的必须要缩进，该对齐的要对齐，该注释的要注释。

在main中先利用单例类创建一个指向配置文件单例类对象的指针，执行load函数，在load函数里打开nginx.conf并处理这个文本文件中的所有内容，具体操作是用打开后逐行读取，删掉不必要的空格。

```c++
CConfig *p_config = CConfig::GetInstance();    //单例类
if(p_config->Load("nginx.conf") == false)    //把配置文件内容载入到内存
{
    printf("配置文件载入失败，退出!\n");
    exit(1);
}


```

**load函数其实就是遍历nginx.conf中的每一行，每一行以等号为中心，构建一个指向如下结构的指针，等号前面的部分放入ItemName中，等号后面的部分放入ItemContent中，再将这个指针push_back入容器m_ConfigItemList结构中。**

```c
typedef struct
{
    char ItemName[50];
    char ItemContent[500];
}CConfItem,*LPCConfItem;
```

（2）内存泄漏的检查工具

Valgrind：帮助程序员寻找程序里的bug和改进程序性能的工具集。擅长发现内存的管理问题，里边有若干工具，其中最重要的是Memcheck（内存检查）工具，用于检查内存的泄漏。

a）memcheck的基本功能，能发现如下的问题；

i）使用未初始化的内存

ii）使用已经释放了的内存

iii）使用超过malloc()分配的内存

iv）对堆栈的非法访问

**v）申请的内存是否有释放**

vi）malloc/free，new/delete申请和释放内存的匹配

vii）memcpy()内存拷贝函数中源指针和目标指针重叠

b）内存泄漏检查示范

config.mk要把debug选项设为true，表示编译时是否生成调试信息。很多调试工具，包括Valgrind工具集都会因为这个为true能够输出更多的调试信息，但是商业运行的代码为了效率要把这一项设为false。

【格式】

**valgrind --tool=memcheck 一些开关可执行文件名**

--tool=memcheck：使用valgrind工具集中的memcheck工具

--leak-check=full：指的是完全检查内存泄漏（显示的多一点）

--show-reachable=yes：是显示内存泄漏的地点

--trace-children=yes：是否跟入子进程

--log-file=log.txt：将调试信息输出到log.txt，不输出到屏幕

**最终用的命令：valgrind --tool=memcheck --leak-check=full --show-reachable=yes  ./nginx**

查看内存泄漏的三个地方:

a）9 allocs，8 frees 差值是1，就没泄漏，超过1就有泄漏；

b）中间诸如：by 0x401363：CConfig::Load(char const *)(ngx_c_conf.cxx：77)和我们自己的源代码有关的提示，就要注意：

c）LEAK SUMMARY: definiely lost：1，100 bytes in 2 blocks（因为一个结构是550字节）

（3）设置可执行程序的标题（名称）

目的：以后主进程和子进程不能叫一个名儿啊

**argc：命令行参数的个数；**

**argv：是个数组，每个数组元素都是指向一个字符串的char*，里边存储的内容是所有命令行参数。**

比如：键入./nginx -v -s 5

则argc = 4， argv[0] = ./nginx， argv[1] = -v，argv[2] = -s，argv[3] = 5;

argv内存之后，接着连续的就是环境变量参数信息内存，也就是可执行程序执行时有关的所有环境变量参数信息，换句话说，environ内存和argv内存紧紧的挨着，会遇到如下问题：

**【问题】如果改名的名称太长，可能会覆盖参数后的环境变量，就出问题了！**

【**解决方法】**

第一步：重新分配一块内存，用来保存environ中的内容（把环境变量这些搬走）；

第二步：修改argv[0]所指向的内存。

搬家主要代码如下：

```c++
for (i = 0; environ[i]; i++) 
{
    size_t size = strlen(environ[i])+1 ; //不要落下+1，否则内存全乱套了，因为strlen是不包括字符串末尾的\0的
    strcpy(ptmp,environ[i]);      //把原环境变量内容拷贝到新地方【新内存】
    environ[i] = ptmp;            //然后还要让新环境变量指向这段新内存
    ptmp += size;
}
```

3、日志打印实战，优化main函数调用顺序

（1）基础设施之日志打印实战代码一

ngx_printf.cxx：放和打印格式相关的函数

ngx_log.cxx：放和日志相关的函数

ngx_log_stderr()：往屏幕上打印一条错误信息，功能类似于printf，可以支持任意自己想支持的格式化字符，%k，%z等。

（2）设置时区

（3）基础设施之日志打印实战代码二

a）日志等级划分

划分日志等级，一共分8级，分级的目的是方便管理，显示，过滤等等；日志级别从高到低，数字最小的级别最高，数字最大的级别最低。

```c
#define NGX_LOG_STDERR            0    //控制台错误【stderr】：最高级别日志，日志的内容不再写入log参数指定的文件，而是会直接将日志输出到标准错误设备比如控制台屏幕
#define NGX_LOG_EMERG             1    //紧急 【emerg】
#define NGX_LOG_ALERT             2    //警戒 【alert】
#define NGX_LOG_CRIT              3    //严重 【crit】
#define NGX_LOG_ERR               4    //错误 【error】：属于常用级别
#define NGX_LOG_WARN              5    //警告 【warn】：属于常用级别
#define NGX_LOG_NOTICE            6    //注意 【notice】
#define NGX_LOG_INFO              7    //信息 【info】
#define NGX_LOG_DEBUG             8    //调试 【debug】：最低级别
```

b）配置文件中和日志有关的选项

void ngx_log_init()：打开/创建日志文件

ngx_log_error_core()：写日志文件的核心函数

最终都是调用ngx_vslprintf函数拼出一个字符串写到日志文件里。

（4）捋顺main函数中代码执行顺序

a）无伤大雅也不需要释放的最上边，比如取得进程ID；

b）初始化失败，就要直接退出的，配置文件必须最先要，后边初始化啥的都用，所以先把配置读出来，供后续使用；

**【特别提醒】**main中exit和return效果差不多，exit(0)表示程序正常，exit(1)/exit(-1)表示程序异常退出，exit(2)表示系统找不到指定的文件。

c）一些初始化函数，比如日志初始化；

d）一些不好归类的其他类别的代码，比如环境变量搬家；

e）该释放的资源要释放掉，main函数返回前写一个freeresource()，用来写一系列释放动作函数。

## 3、信号、子进程实战，文件IO详谈

（1）信号功能实战

ngx_init_signals用来注册信号处理程序（在日志初始化后），注意用sigaction不用signal，然后往该nginx进程发信号就不会执行默认的kill等动作，会执行ngx_signal_handle这个信号处理程序中的内容。

**具体做法是遍历一个signal[]数组，用系统定义的sigaction结构来暂存信号处理函数，用类似sigemptyset(&sa.sa_mask)这种代码来设置屏蔽信号集合为空，也就是不屏蔽任何其他信号，然后用sigaction函数来绑定对应的信号和信号处理函数。**

（2）nginx中创建worker子进程

官方nginx，一个master进程，创建了多个worker子进程；fork的子进程数目一般与CPU数目相同。

**【代码流程】**

```c++
ngx_master_process_cycle()        //创建子进程等一系列动作
    ngx_setproctitle()            //设置进程标题    
    ngx_start_worker_processes()  //创建worker子进程   
        for (i = 0; i < threadnums; i++)   //master进程在走这个循环，来创建若干个子进程
            ngx_spawn_process(i,"worker process");
                pid = fork();     
                   //分叉，从原来的一个master进程分成两个叉（原有的master进程，以及一个新fork()出来的worker进程
                //只有子进程这个分叉才会执行ngx_worker_process_cycle()
                ngx_worker_process_cycle(inum,pprocname);  //子进程分叉
                    ngx_worker_process_init();
                        sigemptyset(&set);  
                        sigprocmask(SIG_SETMASK, &set, NULL); //允许接收所有信号
                        ngx_setproctitle(pprocname);          //重新为子进程设置标题为worker process
                        for ( ;; ) {}. ....                   //子进程开始在这里不断的死循环
    sigemptyset(&set); 
    for ( ;; ) {}.                //父进程[master进程]会一直在这里循环


```

**调试技巧：kill -9 -1344，用负号+组id，可以杀死一组进程。**

sigsuspend()函数讲解：阻塞在这里等待一个信号，此时进程是挂起的，不占用cpu时间，只有收到信号才会被唤醒（返回），**这是一个原子操作**。因为这个函数是处理信号的，所以适合管理进程master，不适合worker进程。

a）根据给定的参数设置新的mask并阻塞当前进程【因为是个空集，所以不阻塞任何信号】；

b）此时，一旦收到信号，便恢复原先的信号屏蔽【阻塞了多达10个信号】；

c）调用该信号对应的信号处理函数（已经阻塞了好多信号，所以不会被打断）；

d）信号处理函数返回后，sigsuspend返回，使程序流程继续往下走。

（3）日志输出重要信息谈

a）换行回车进一步示意

\r：回车符，把打印【输出】信息的为止定位到本行开头

\n：换行符，把输出为止的移动到下一行

一般把光标移动到下一行的开头用\r\n；如windows下，每行结尾\r\n;类Unix，每行结尾就只有\n。

**结论：统一用\n！**

b）printf()函数不加\n无法及时输出的解释

**printf()末尾不加\n就无法及时的将信息显示到屏幕**。这是因为**行缓存**（类Unix上才有），需要输出的数据不直接显示到终端，而是首先缓存到某个地方，当遇到行刷新表指或者该缓存已满的情况下，才会把缓存的数据显示到终端设备；

ANSI C中定义\n认为行刷新标记，所以printf函数没有带\n是不会自动刷新输出流，直至行缓存被填满才显示到屏幕上。

**所以用printf的时候，注意末尾要用\n！**

其他两种解决方法：

```c
fflush(stdout); //刷新标准输出缓冲区，把输出缓冲区里的东西打印到标准输出设备上，则printf里的东西会立即输出。
setvbuf(stdout,NULL,_IONBF,0); //这个函数直接将printf缓冲区禁止，printf就直接输出了。
```

（4）write()函数思考

【测试】多个进程同时去写一个文件，比如5个进程同时往日志文件中写，会不会造成日志文件混乱。多个进程同时写一个日志文件，可以看到输出结果并不混乱，是有序的；我们的日志代码应对多进程日志文件中写时没有问题。为什么？

a）多个进程写一个文件（如果没有父子关系），可能会出现数据覆盖，混乱等情况，也就是说**如果是每个进程都open打开后然后往里面写就会出问题；**

b）ngx_log.fd = open((const char*) plogname, O_WRONLY | O_APPEND| O_CREAT,0644)

O_APPEND这个标记能够保证多个进程操作同一个文件时不会相互覆盖

c）内核write()写入时是原子操作；

d）父进程fork()子进程是亲缘关系，会共享文件表项。

是否数据成功被写到磁盘？write成功了不等于真写到磁盘了！

write()调用返回时，内核已经将应用程序缓冲区所提供的数据放到了内核缓冲区，但是无法保证数据已经写出到其预定的目的地【磁盘】。的确，因为write()调用速度极快，可能没有时间完成该项目的工作【实际写磁盘】就返回了，所以这个write()调用不等价于数据在内核缓存区和磁盘之间的数据交换。

【掉电导致write()的数据丢失破解法】

a）直接I/O（效率很低）：直接访问物理磁盘，open的时候选择O_DIRECT，表示绕过内核缓存区。

b）open文件时用O_SYNC选项：

同步选项【把数据直接同步到磁盘】，只针对write函数有效，使每次write()操作等待物理I/O操作的完成；

具体说，就是将写入内核缓冲区的数据立即写入磁盘，将掉电等问题造成的损失减到最小；每次写磁盘数据，务必要大块大块写，一般都512-4K 4K的写。

c）缓存同步：尽量保证缓存数据和写到磁盘上的数据一致（推荐！）

sync(void)：将所有修改过的块缓冲区排入写队列；然后返回，并不等待实际写磁盘操作结束，数据是否写入磁盘并没有保证；

fsync(int fd)：将fd对应的文件的块缓冲区立即写入磁盘，并等待实际写磁盘操作结束返回；

fdatasync(int fd)：类似于fsync，但只影响文件的数据部分。而fsync不一样，fsync除数据外，还会同步更新文件属性。

建议：多次write，每次write建议都4k，然后调用一次fsync()。

（5）标准I/O库

fwrite和write有啥区别；

fwrite()是标准I/O库，一般在stdio.h文件中，write()是系统调用，操作系统调用的；

所有系统调用都是原子性的。

## 4、守护进程及信号处理实战

（1）守护进程功能的实现

【回顾】避免bash关闭时进程也关闭了，两种解决方法：

a）拦截掉SIGHUP，那么终端窗口关闭，进程就不会跟着关闭；

b）守护进程，三章七节，一运行就在后台，不会占着终端。

创建守护进程ngx_daemon（）、work()子进程创建之前调用ngx_daemon()，这样创建的子进程也都是守护进程了。

创建守护进程后观察：

a）1个master进程，4个worker进程，状态S，表示休眠状态，但没有+，说明这几个进程不在前台进程组；

b）master进程的ppid是1【老祖宗进程init】，其他几个worker进程的父进程都是master；

c）tt这列都为？，表示他们脱离了终端，不与具体的终端挂钩了；

d）进程组PGRP都相同。

【结论】

a）守护进程如果通过键盘执行可执行文件来启动，虽然守护进程与具体终端是脱钩的，但是依旧可以往标准错误上输出内容，这个终端对应的屏幕上可以看到输入的内容；

b）如果这个nginx守护进程不是通过终端启动的，可能开机就启动，那么这个nginx守护进程就完全无法往任何屏幕上显示信息了，这个时候，要排错就要靠日志文件了；

（2）信号处理函数的进一步完善

避免子进程被杀掉时变成僵尸进程：父进程要处理SIGCHILD信号并在信号处理函数中调用waitpid()来解决僵尸进程的问题。

# 第四章 网络通讯实战一

## 1、C/S，TCP/IP协议妙趣横生、惟妙惟肖谈

（1）客户端与服务器

浏览器就是一个可执行程序（客户端），淘宝网nginx服务器返回数据包，来回很多次才完全发完，最后发一个特殊的包结束。

![](E:\jianguoyun\我的坚果云\nginx\20190408205651734.png)

【客户端服务器角色规律总结】

a）数据通讯总在两端进行，其中一端叫客户端，另一端叫服务器；

b）总有一方先发起第一个数据包，这发起第一个数据包的这一端，就叫客户端【浏览器】；被动收到第一个数据包这端，叫服务器端【淘宝服务器】；

c）连接建立起来，数据双向流动（双工）

d）既然服务器端是被动接收连接，那么客户端必须得能够找到服务器在哪里；

浏览器要访问淘宝网，需要知道淘宝服务器的地址（IP地址），以及淘宝服务器的姓名（端口号，一个0-65535的无符号数字）

淘宝网服务器（nginx服务器）会调用listen()函数来监听80端口。

**在编写网络通讯程序时，只需要指定淘宝服务器的ip地址和淘宝服务器的端口号，就能够跟淘宝服务器进行通讯。**

e）epoll：单台服务器高并发

（2）网络模型

a）OSI七层网络模型

物【物理层】链【数据链路层】网【网络层】传【传输层】会【会话层】表【表示层】应【应用层】

把一个要发送出去的数据包从里到外裹了7层发送到网络上去。

b）TCP/IP协议四层模型

tcp/ip实际上是一组协议的代名词，而不仅仅是一个协议；每一层都对应着一些协议。

![](E:\jianguoyun\我的坚果云\nginx\20190408211234103.png)

c）TCP/IP协议的解释和比喻

上层数据先加TCP，再加IP头，再加LLC头，再加MAC头。

![](E:\jianguoyun\我的坚果云\nginx\20190408212106990.png)

（3）最简单的客户端和服务器程序实现代码

a）套接字socket概念

套接字（socket）：就是个数字，通过调用socket()函数来生成；这个数字具有唯一性；一直用直到调用close()函数把这个数字关闭；一切皆文件，咱们就把socket也堪称是文件描述符，我们可以用socket来收发数据；send()，recv()。

b）一个简单的服务器端通讯程序范例

![](E:\jianguoyun\我的坚果云\nginx\20190408212715717.png)

c）IP地址

写服务器程序，不用考虑ipv4、ipv6的问题，遵照ipv4规则写就行;

写客户端程序，只演示ipv4版本的客户端范例。

d）一个简单的客户端通讯程序范例

c/s建立连接时双方彼此都要有ip地址/端口号；连接一旦建立起来，那么双方的通讯【双工收发】就只需要用双方彼此对应的套接字即可。

e）客户端服务器综合演示和调用流程图

服务器端程序要先运行

（4）TCP和UDP的区别

TCP(Transfer Control Protocol)：传输控制协议

UDP(User Datagram Protocol)：用户数据报协议

TCP协议：可靠的面向连接的协议，数据包丢失的话操作系统底层会感知并且帮助你重新发送数据包；

UDP协议：不可靠的，无连接的协议。

【优缺点】

a）tcp：可靠协议，耗费更多的系统资源确保数据传输的可靠；得到的好处就是只要不断线，传输给对方的数据，一定正确的，不丢失，不重复，按顺序到达对端；

b）udp：不可靠协议，发送速度特别快；但无法确保数据可靠性。

【各自的用途】

a）tcp：文件传输，收发邮件需要准确率高，但效率可以相对较差；一般TCP比UDP用的范围和场合更广。

b）udp：qq聊天信息，DNS解析等，估计随着网络的发展，网络性能更好，丢包率更低，那么udp应用范围更广。

## 2、TCP三次握手详析、telnet、wireshark示范

（1）TCP连接的三次握手

只有TCP有三次握手（UDP没有）

a）最大传输单元MTU

MTU：每个数据包包含的数据最多可以有多个个字节，1.5K左右；

要发送100K，操作系统内部会把这100K数据拆分成若干个数据包（分片），每个数据包大概1.5K之内（大概拆解成68个），对端重组；我们只需要知道有拆包，组包，这68个包各自传递的路径可能不同，每一个包可能因为路由器，交换机原因可能被再次分片；最终TCP/IP协议保证了我们收发数据的顺序性和可靠性。

b）TCP包头结构（往左90度）

i）源端口、目的端口

ii）关注syn位和ack位（开/关）

iii）一个tcp数据包，是可能没有包体，此时，总会设置一些标志位来达到传输控制信息的目的，起控制作用

c）TCP数据包收发之前的准备工作

TCP数据的收发是双工的：每端既可以收数据，又可以发收据。

【TCP数据包的收发三大步骤】

i）建立TCP连接[connect：客户端]，三次握手

ii）多次反复的数据收发[read/write]

iii）关闭TCP连接[close]

**UDP不存在三次握手来建立连接的问题。UDP数据包是直接发送出去，不用建立所谓的连接**。

d）TCP三次握手建立连接的过程（[syn] [syn/ack] [ack]）

![](E:\jianguoyun\我的坚果云\nginx\20190409112858294.png)

i）客户端给服务器发送了一个SYN标志位置位的无包体TCP数据包，SYN被置位，就表示发起TCP连接；

ii）服务器收到了这个SYN标志位置位的数据包，服务器给客户端返回一个SYN和ACK标志位都被置位的无包体TCP数据包；

iii）客户端收到服务器发送回来的数据包之后，再次发送ACK置位的数据包，服务器收到这个数据包之后，客户端和服务器的TCP连接就正式建立。

e）为什么TCP握手是三次握手而不是二次

三次握手很大程度上是为了防止恶意的人坑害别人而引入的一种TCP连接验证机制。

为了确保数据稳定可靠的收发，尽量减少伪造数据包对服务器的攻击。

源ip，源端口 -----------------目的ip，目的端口

syn ------------>

<--------------syn/ack（验证源ip和源端口真实存在）

ack------------>

（2）telnet工具使用介绍

telnet是一款命令行方式运行的客户端TCP通讯工具，可以连接到服务器端，往服务器端发送数据，也可以接收从服务器发送过来的信息；**该工具能够方便地测试服务器的某个TCP端口是否通**，是否能够正常收发数据，所以是一个非常实用，常用的工具。

```shell
telnet ip地址 端口号
```

telnet在windows下输入一个字符就往server发，在linux下输入字符后要回车才往server发。

（3）wireshark监控数据包

wireshark是个软件，分析网络数据包，规则：host 192.168.1.126 and port 9000。

![](E:\jianguoyun\我的坚果云\nginx\20190409120336339.png)

【TCP断开的四次挥手】

a）FIN、ACK   服务器->客户端

b）ACK            客户端-> 服务器

c）FIN、ACK   客户端-> 服务器

d）ACK            服务器->客户端

![](E:\jianguoyun\我的坚果云\nginx\20190409120821969.png)

## 3、TCP状态转换，TIME_WAIT详解，SO_REUSEADDR

（1）TCP状态转换

同一个IP（INADDR_ANY），同一个端口SERV_PORT，只能被成功的bind()一次，若再次bind()就会失败，并且会显示：Address already in use。

介绍命令netstat：显示网络相关信息

-a：显示所有选项

-n：能显示成数字的内容全部显示成数字

-p：显示段落对应的程序名

```shell
netstat -anp | grep -E 'State|9000'
```

【测试】用两个客户端连接服务器，服务器给每个客户端发送一串"I sent sth to client!\n"，并关闭客户端；我们用netstat观察，原来那个监听端口一直在监听，但是当来了两个连接之后（连接到服务器的9000端口），虽然这两个连接被close掉了，但是产生了两条TIME_WAIT状态的信息。

客户端连接到服务器，并且服务器把客户端关闭，服务器就会产生一条针对9000监听端口的状态为TIME_WAIT的连接；只要用netstat看到TIME_WAIT状态的连接，那么此时杀掉服务器程序再重新启动，就会启动失败，bind()函数返回失败。

TCP状态转换图（11种状态）是针对一个TCP连接来说的。

客户端：CLOSE -> SYN_SENT -> ESTABLISHED 【连接建立，可以进行数据收发】

服务端：CLOSE -> LISTEN -> 【客户端来握手】SYN_RCVD -> ESTABLISHED 【连接建立，可以进行数据收发】

谁主动close连接，谁就会给对方发送一个FIN标志置位的一个数据包给对方，假设是服务器先关闭：

服务器主动关闭连接：ESTABLISHED -> FIN_WAIT1 -> FIN_WAIT2 ->TIME_WAIT（等一段时间回到close状态）

客户端被动关闭：ESTABLISHED -> CLOSE_WAIT -> LAST_ACK

![](E:\jianguoyun\我的坚果云\nginx\20190409182319161.png)

（2）TIME_WAIT状态

**【何时产生】四次挥手主动关闭的一方就会产生！**

具有TIME_WAIT状态的TCP连接，就好像一种残留的信息一样，服务器程序退出并重新执行会失败；连接处于TIME_WAIT状态是有时间限制的（1-4分钟之间） = 2 MSL（最长数据包生命周期）。

【为什么要引入TIME_WAIT】：

a）可靠的实现TCP全双工的终止

**如果服务器最后发送的ACK包因为某种原因丢失了，那么客户端一定会重新发送FIN，这样因为服务端有TIME_WAIT的存在，服务器会重新发送ACK包给客户端，但是如果没有TIME_WAIT这个状态，服务器都已经关闭连接了，此时客户端重新发送FIN，服务器给回的就不是ACK包，而是RST（连接复位）包，从而使客户端没有完成正常的4次挥手，可能出现错误；**

RST标志：当我们close一个TCP连接时，如果发送缓冲区有数据，那么操作系统会很优雅的把发送缓存区里的数据发送完毕，然后再发FIN包表示连接关闭；出现这个RST标志的包一般都表示异常关闭，如果发生了异常，一般都会导致丢失一些发送缓冲区的数据包，**主动关闭一方也不会进入TIME_WAIT。**

b）允许老的重复的TCP数据包在网络中消逝

如果最后一个ack还没到客户端，不用TIME_WAIT阻止新server启动，这时启动一个server，再来一个老数据包，就不知道这个包是给老的还是给新的了，容易引起混乱。**所以维持TIME_WAIT一段时间是为了在这段时间内彻底丢弃老的数据包，等待最后一个ack到达客户端，再全部结束。**

（3）SO_REUSEADDR选项

setsockopt（SO_REUSEADDR）用在服务器端，socket()创建之后，bind()之前。

**所有TCP服务器都应该指定本套接字选项，以防止套接字处于TIME_WAIT时bind()失败的情形出现。**

【设置后测试】两个进程，绑定同一个IP和端口：bind()失败；TIME_WAIT状态时的bind绑定：bind()成功。

**SO_REUSEADDR：主要解决TIME_WAIT状态bind()失败的问题**

## 4、listen()队列剖析、阻塞非阻塞、同步异步

（1）listen()队列剖析

listen()：监听端口，用在TCP连接中的服务器端角色。

```c++
listen()函数调用格式：int listen(int sockfd, int backlog);
```

要理解好backlog这个参数，要了解"监听套接字队列"!

a）监听套接字的队列

对于一个调用listen()进行监听的套接字，操作系统会给这个套接字维护两个队列：

i）未完成连接队列（保存连接用的）

当客户端发送tcp连接三次握手的第一次（syn包）给服务器的时候，服务器就会在未完成队列中创建一个跟这个syn包对应的一项，我们把这项看成是一个半连接（因为连接还没建立），这个半连接的状态会从LISTEN变成SYN_RCVD状态，同时给客户端返回第二次握手包（syn，ack包），这个时候，其实服务器是在等待完成第三次握手。

ii）已完成连接队列（保存连接用的）

当第三次握手完成了，这个连接就变成了ESTABLISHED状态，每个已经完成三次握手的客户端都放在这个队列中作为一项。

![](E:\jianguoyun\我的坚果云\nginx\20190410153828718.png)

backlog曾经的含义：已完成队列和未完成队列里边条目之和不能超过backlog。

![](E:\jianguoyun\我的坚果云\nginx\20190410154110708.png)

【总结】

i）客户端这个connect()什么时候返回？收到三次握手的第二次握手包（也就是收到服务发回来的syn/ack）之后返回。

ii）RTT是未完成队列中任意一项在未完成队列中留存的时间，这个时间取决于客户端和服务器。

对于客户端，这个RTT时间是第一次和第二次握手加起来的时间；对于服务器，这个RTT时间实际上是第二次和第三次握手加起来的时间。如果这三次握手包传递速度特别快的话，大概187毫秒能够建立起来这个连接，这个时间不快，所以建立TCP连接的成本很高。

iii）如果一个恶意客户，迟迟不发送三次握手的第三个包。那么这个连接就建立不起来，那么这个处于SYNC-RCVD的这一项，就会一致停留在服务器的未完成队列中，**这个停留时间大概是75秒**，如果超过这个时间，这一项会被操作系统干掉。

b）accept函数()

accept()函数：**就是用来从已完成连接队列中的队首位置取出来一项（每一项都是一个已经完成三路握手的TCP连接）返回给进程**。如果已完成连接队列是空，那么accept()一般会一直卡在这里（休眠）等待，一直到已完成队列中有一项时才会被唤醒。因为客户端connect返回就可以收发数据了，所以服务器端要尽快地用accept()把已完成队列中的数据（TCP连接）取走。

accept()返回的是个套接字，这个套接字就代表那个已经用三次握手建立起来的那个tcp连接，因为accept()是从已完成队列中取的数据，所以一定要区别服务器端的两个套接字：

i）监听9000端口这个套接字 ----- "监听套接字"（listenfd），只要服务器程序在运行，这个套接字就应该一直存在；

ii）当客户端连接进来，操作系统会为每个成功建立三次握手的客户端再创建一个套接字（connfd），accept()返回的就是这种套接字，也就是从已完成连接队列中取得的一项。随后服务器使用这个accept()返回的套接字和客户端通信的。

【思考题】

i）如果两个队列之和（已完成连接队列和未完成连接队列）达到了listen()所指定的第二参数，也就是说队列满了，此时，再有一个客户发送syn请求，服务器怎么反应？

**实际上服务器会忽略这个syn，不给回应；客户端这边，发现syn没回应，过一会会重发syn包。**

ii）从连接被扔到已完成队列中去，到accept()从已完成队列中把这个连接取出之间是有个时间差的，如果还没等accept()从已完成队列中把这个连接取走的时候，客户端如果发送来数据，**这个数据就会被保存在已经连接的套接字的接收缓冲区里，这个缓冲区有多大，最大就能接收多少数据量。**

c）syn攻击（syn flood）：典型的利用TCP/IP协议涉及弱点进行坑爹的一种行为。

拒绝服务攻击（DOS/DDOS）：**不停发第一次握手的syn包，服务器未完成队列中就越来越多**，总会超过backlog；

所以backlog被进一步明确和规定了：**已完成连接队列中最大条目数**。虽然这样即使未完成连接队列被填满后还是无法处理新的连接，但是系统会自动处理未完成连接队列的内容，75s后删除。

所以在写代码时尽快用accept()把已经完成队列里边的连接取走，尽快留出空闲位置给后续的已完成三次握手的条目用，这个已完成队列就一般不会满，一般这个backlog值给300左右。

（2）阻塞与非阻塞I/O

阻塞和非阻塞主要是指调用某个系统函数时，这个函数是否会导致我们的进程进入sleep()（卡住休眠）状态而言的。

![](E:\jianguoyun\我的坚果云\nginx\2019041017221444.png)

a）阻塞I/O

调用一个函数，这个函数卡在这里等待一个事情发生，只有这个事情发生了，这个函数才会往下走，如阻塞函数accept()。这种阻塞并不好，效率很低，所以一般不会用阻塞方式来写服务器程序。

![](E:\jianguoyun\我的坚果云\nginx\2019041017040354.png)

b）非阻塞I/O：不会卡住，充分利用时间片，执行效率更高。

【非阻塞模式的两个鲜明特点】

i）不断地调用accept()、recvfrom()函数来检查有没有数据到来，如果没有，函数会返回一个特殊的错误标记来标识，这种标记可能是EWOULDBLOCK，也可能是EAGAIN；如果数据没到来，那么这里有机会执行其他函数，但是也得不停地再次调用accept()，recvfrom()来检查数据是否到来，非常累；

ii）如果数据到来，那么就得卡在这里把数据从内核缓冲区复制到用户缓冲区，所以**复制这个阶段是卡着完成的**。

![](E:\jianguoyun\我的坚果云\nginx\20190410171225985.png)

（3）同步与异步I/O：这两个概念容易和阻塞/非阻塞混淆

a）异步I/O：

![](E:\jianguoyun\我的坚果云\nginx\20190410172304934.png)

调用一个异步I/O函数时，要给这个函数指定**一个接收缓冲区**，我还要给定**一个回调函数**；

**调用完一个异步I/O函数后，该函数会立即返回，其余判断交给操作系统，操作系统会判断数据是否到来，如果数据到来了，操作系统会把数据拷贝到你所提供的缓冲区里，然后调用你所指定的这个回调函数来通知你。**

【非阻塞和异步I/O区别】

i）非阻塞I/O要不停地调用I/O函数来检查数据是否到来，如果数据来了，就得卡在I/O函数这里把数据从内核缓冲区复制到用户缓冲区，然后这个函数才能返回；

ii）异步I/O根本不需要不停地调用I/O函数来检查数据是否到来，只需要调用一次，然后就可以干别的事情去了；内核判断数据到来，拷贝数据到你提供的缓冲区，调用你的回调函数来通知你，并没有被卡在那里的情况。

b）同步I/O：select/poll和epoll

![](E:\jianguoyun\我的坚果云\nginx\20190410172759888.png)

步骤一：调用select()判断有没有数据，有数据，走下来，没数据卡在那里；

步骤二：select()返回之后，用recvfrom()去取数据，当然取数据的时候（内核到用户）也会卡那么一下。

同步I/O感觉更麻烦，要调用两个函数才能把数据拿到手；但是同步I/O和阻塞式I/O比，有I/O复用（2个函数的）能力。**所谓I/O复用，就是多个socket（多个TCP连接）可以弄成一捆，我可以用select等数据，因为select()的能力是等多条TCP连接上的任意一条有数据来都可以感知到，然后哪条TCP有数据来，我再用具体的比如recvfrom()去收。**

所以，这种调用一个函数能够判断一堆TCP连接是否来数据的这种能力，叫I/O复用，英文I/O multiplexing。

很多资料把阻塞I/O，非阻塞I/O，同步I/O归结为一类，因为他们多多少少都有阻塞的行为发生；也可以把阻塞I/O，非阻塞I/O都归结为同步I/O模型，而把异步I/O单独归结为一类，因为异步I/O是真正的没有阻塞行为发生的。

【通俗理解】参考：https://www.cnblogs.com/wangzhaobo/articles/9596623.html

阻塞与非阻塞关注的是在**等待数据来的时候是不是在干别的事（这里的等应该指第一阶段）**，而同步与异步关注的是如何获取到数据来了这件事，**是自己看到的还是别人告诉的（其实可以把看这个动作理解成第二阶段）**，所以阻塞和非阻塞都算有一点同步。现实中非阻塞异步是最合我们常理的，好比我们去买煎饼，我跟摊煎饼的阿姨说，你摊好了给我打电话（不用自己看，所以是异步的），然后我就去聊天、玩手机（程序先返回干别的，非阻塞），然后摊好了通知我（回调函数），我取走煎饼。最蠢的其实就是阻塞异步地做事，就好比你跟阿姨说煎饼好了你来通知我一下，然后你还在那里一直等着数据来而不是干别的事（阻塞），那你让阿姨通知你干嘛？同理，阻塞同步就是一直站在那里等煎饼出来，然后看（同步）到煎饼就拿走，因为是自己看而不是等通知所以是同步。非阻塞同步就是去聊天玩手机，然后每一会就过来看一下煎饼摊好没有，摊好了就拿走，因为总归是自己看的，所以是同步。

# 第五章 网络通讯实战二

## 1、监听端口实战、epoll介绍及原理祥析

（1）监听端口

在创建worker进程之前就要执行函数ngx_open_listening_sockets()，先创建套接字，再setsockopt防止TIME_WAIT，再ioctl设置非阻塞，然后listen，bind，将监听的套接字放入m_ListenSocketList容器。

（2）epoll技术概述

a）I/O多路复用：epoll就是一种典型的I/O多路复用技术，epoll技术的最大特点是支持高并发；

传统多路复用技术select，poll，在并发量达到1000-2000性能就会明显下降；epoll，从linux内核2.6引入的，2.6之前是没有的。

b）epoll和kqueue技术类似：单独一台计算机支撑少则数万，多则数十上百万并发连接的核心技术。

epoll技术完全没有性能会随着并发量提高而出现明显下降的问题。但是并发每增加一个，必定要消耗一定的内存去保存这个连接线管的数据；并发量总还是有限制的，不可能是无限的。

c）10万个连接同一时刻，可能只有几十上百个客户端给你发送数据，epoll只处理这几十上百个客户端。

d）很多服务器程序用多进程，每一个进程对应一个连接；也有用多线程做的，每一个线程对应一个连接；

epoll事件驱动机制，在单独的进程或者单独的线程里运行，收集/处理事件；没有进程/线程之间切换的消耗，高效。

e）适合高并发，开发难度极大。

（3）epoll原理与函数介绍：三个函数（操作系统提供的，只会用就行）

a）epoll_create()函数

```c
int epoll_create(int size);    //size>0就行
```

【功能】创建一个epoll对象，返回该对象的文件描述符，这个描述符就代表这个epoll对象，最终要用close()关闭。

【原理】

i）struct eventpoll *ep =（struct eventpoll*）calloc(1,sizeof(struct eventpoll));分配一段内存

ii）rbr结构成员：代表一颗红黑树的根节点[刚开始指向空]把rbr理解成红黑树的根节点的指针；

**红黑树，用来保存键（数字）/值（结构），能够快速地通过key取出键值对。**

![](E:\jianguoyun\20190411120504968.png)

iii）rdlist结构成员：代表一个双向链表的表头指针；

**双向链表：从头访问/遍历每个元素特别快，一直next就行。**

![](E:\jianguoyun\20190411120620879.png)

iv）总结：创建了一个eventpoll结构对象，被系统保存起来：

rbr成员被初始化成指向一颗红黑树的根（有了一个红黑树）；

rdlist成员被初始化成指向一个双向链表的根（有了双向链表）。

b）epoll_ctl()函数

```c
int epoll_ctl(int efpd,int op,int sockid,struct epoll_event *event);
```

【功能】把一个socket以及这个socket相关的事件添加到这个epoll对象描述符中去，目的就是通过这个epoll对象来监视这个socket（客户端的TCP连接）上数据的来往情况，当有数据来往时，系统会通知我们；**我们把感兴趣的事件通过epoll_ctl()添加到系统，当这些事件来的时候，系统会通知我们。**

【字段说明】

efpd：epoll_create()返回的epoll对象描述符；

op：动作，添加/删除/修改，对应数字是1，2，3，**EPOLL_CTL_ADD、EPOLL_CTL_DEL、EPOLL_CTL_MOD。**

EPOLL_CTL_ADD：添加事件，往红黑树上添加一个节点，每个客户端连入服务器后，服务器都会产生一个对应的socket，对应每个连接的这个socket值肯定都不重复；这个socket就是红黑树中的key，把这个节点（其实是一个结构）挂到红黑树上去。

EPOLL_CTL_MOD：修改事件，用了EPOLL_CTL_ADD把节点添加到红黑树上之后才存在修改。

EPOLL_CTL_DEL：是从红黑树上把这个节点干掉，这会导致这个socket（这个tcp连接）上无法收到任何系统通知事件。

sockid：想读时就是监听套接字，想写时就是从accept()返回的sockfd，这个sockid也就是红黑树里边的key。

event：事件信息，EPOLL_CTL_ADD和EPOLL_CTL_MOD都要用到这个event参数里边的事件信息。

【原理】假设ADD，先判断有没有这个key，生成一个epitem对象（一个结点），把socket和事件保存在结点中，把结点加入红黑树。每一个epi就是一个指向epitem的指针，rbn中有三个指针，分别指向左孩子、右孩子和父亲。

![](E:\jianguoyun\20190411143954328.png)

epi = RB_INSERT(_epoll_rb_socket, &ep->rbr, epi); 【EPOLL_CTL_ADD】，增加节点到红黑树中；

epi = RB_REMOVE(_epoll_rb_socket, &ep->rbr, epi); 【EPOLL_CTL_DEL】，从红黑树中把节点干掉；

EPOLL_CTL_MOD，找到红黑树节点，修改这个节点中的内容。

【面试】

EPOLL_CTL_ADD：等价于往红黑树中增加节点

EPOLL_CTL_DEL：等价于从红黑树中删除节点

EPOLL_CTL_MOD：等价于修改已有的红黑树的节点

**当事件发生，我们如何拿到操作系统的通知？**

c）epoll_wait()函数

```c++
int epoll_wait(int epfd,struct epoll_event *events,int maxevents,int timeout);
```

【功能】阻塞一小段时间并等待事件发生，返回事件集合，也就是获取内核的事件通知。

**因为双向链表里记录的是所有数据/有事件的socket（TCP连接），所以具体做法是遍历这个双向链表，把这个双向链表里边的节点数据拷贝出去，拷贝完毕的就从双向链表里移除。**

【字段说明】

epfd：epoll_create()返回的epoll对象描述符；

events：是内存，也是数组，长度是maxevents，表示此次epoll_wait调用可以收集到maxevents个已经就绪【已经准备好的】的读写事件，也就是返回的是实际发生事件的tcp连接数目。

参数timeout：阻塞等待的时长，没数据等待这么久，有数据直接返回；

epitem结构涉及的高明之处：既能够作为红黑树中的节点，又能够作为双向链表中的节点（rdlink）。

epoll_wait的作用就是从双向链表中把所有事件发生的连接取出来再read或write等，只有发生某个事件的连接才在双向链表中。

![](E:\jianguoyun\20190411150400616.png)

【原理】等一段时间，这段时间是用来把sockfd扔到双向链表中，取得事件数量（给的空间和来的数量取小的），每次从双向链表头中一个一个取（但在红黑树中还存在），rdy = 0表示不在双向链表中，把事件拷贝到提供的events。

d）内核向双向链表增加节点

一般有四种情况，会使操作系统把节点插入到双向链表中：

i）客户端完成三路握手；服务器要accept()从已完成队列中取走连接；

ii）当客户端关闭连接，服务器也要调用close()关闭；

iii）客户端发送数据来的；服务器要调用read()，recv()函数来收数据；

iv）当可以发送数据时；服务器可以调用send()，write()来发送数据。

## 2、通讯代码精粹之epoll函数实战1

（1）配置文件的修改

增加worker_connections项：允许连接的最大并发数（1024）

（2）epoll函数实战

epoll_create()、epoll_ctl()、epoll_wait()系统提供的函数调用

a）ngx_epoll_init函数内容

i）epoll_create()

创建一个epoll对象，创建了一个红黑树，还创建了一个双向链表；

ii）创建连接池

【连接池】就是一个数组，元素数量就是worker_connections（1024）,每个数组元素类型为ngx_connection_t（结构）。

为什么要引入这个数组?2个监听套接字，用户连入进来，每个用户多出来一个套接字（sockfd）；**套接字只是一个数字，把套接字数字跟一块内存捆绑，达到的效果就是将来我通过这个套接字，就能够把这块内存拿出来**。

把数组中的每一项的一个内容指向下一个数组元素，相当于串起来的一个空闲链表，这样当来了一个连接，需要取一个套接字时，就不需要遍历1024个连接找空的了，直接把链表头的连接取出来对应客户端连接，用完了再放回链表头。

iii）遍历所有监听端口，为每个端口分配一个连接池中的连接来对应

调用ngx_get_connection()重要函数从连接池中找空闲连接，非常快！

iv）epoll_ctl的EPOLL_CTL_ADD来往红黑树里加结点

**lsof -i:80 命令可以列出哪些进程在监听80端口**

b）ngx_epoll_init函数的调用（要在子进程中执行）

【具体过程】父进程先监听80和443端口，创建子进程过程中进行epoll相关初始化，调用ngx_epoll_init函数为每个进程开辟一个1024个元素的数组，遍历所有socket，将其对应c[0]（空闲池的一个位置），相当于每个socket都对应了连接池的一项。然后，调ngx_epoll_add_event（里面是epoll_ctl）这种函数来往对应socket上增加事件，可能是读可能是写，然后就不用管了，这样如果有客户端来连接内核就会识别到从epoll_wait中往下走。

具体是怎么走的呢？

**客户端连接请求到来时，内核从红黑树中找对应的文件描述符，将对应的结点（epitem）加入双向链表，然后epoll_wait将改变的事件放入events（epoll_wait返回events），再遍历events调用accept函数完成三次握手。所以对于写程序的人，只需要在子进程中循环调用epoll_wait并对返回的events遍历，调相关的如accept一类针对监听套接字的函数即可。**

## **3、通讯代码精粹之epoll函数实战2**

（1）ngx_epoll_process_events函数调用位置

这个函数，仍旧是在子进程中被调用，被放在了子进程的for(;;)，意味着这个函数会被不断的调用。

（2）ngx_epoll_process_events函数内容

用户三次握手成功连入进来，这个"连入进来"这个事件对于服务器来说，就是一个监听套接字上的可读事件。

（3）ngx_event_accept函数内容

epoll_wait走下来就去执行这个函数去accept，生成用来传数据的服务器套接字！

i）accept4/accept：注意设置成非阻塞；

ii）ngx_get_connection：从连接池中取一项，对新套接字绑定一块内存；

iii）ngx_epoll_add_event：对新套接字绑定处理函数rhandler

**【epoll的两种工作模式：LT和ET】**

LT：level trigged，水平触发，这种工作模式是低速模式（效率差），epoll缺省用此模式。

ET：edge trigged，边缘触发/边沿触发，这种工作模式是高速模式（效率好）

水平触发：**来一个事件，如果你不处理它，那么这个事件就会一直被触发**；具体，因为是水平触发而且worker死循环里调用epoll_wait，所以epoll_wait每次都触发了事件，每次都走下来，如果不用函数处理，每次都提醒你有三次握手连入；

边缘触发：**只对非阻塞socket有用，来一个事件，内核只会通知你一次（不管是否处理，内核都不再次通知）**；边缘触发模式，提高系统运行效率，编码的难度加大；因为只通知一次，所以街道通知后，你必须要保证把该处理的事情处理利索。比如客户端发来了100B的数据，结果只处理了70B，还有30B在收缓冲区中没读，这时，如果是水平触发，就还会收到去读数据的事件，会不停提醒直到读完，但如果是边缘触发的话，自己没处理干净，就得等下次再有别的什么东西触发这个事件去读那30B。

**现状：所有的监听套接字用的都是水平触发；所有的接入进来的用户套接字都是边缘触发。**

小tips：有可能第一个事件是关闭连接的，这时c->fd = -1，然后第三个事件是跟第一个事件操作相同的套接字，这时要判断一下，如果c->fd等于-1就continue，这样就可以过滤过期事件。

（4）总结和测试

a）服务器能够感知到客户端发送过来abc字符了

b）来数据会调用ngx_wait_request_handler()，在屏幕上打印一行22222222(往errno打印)

（5）事件驱动总结：nginx所谓的事件驱动框架（**面试可能问到**）

【**总结事件驱动框架/事件驱动架构**】

所谓的事件驱动框架，九市由一些事件发生源（三次握手内核通知或客户端发来数据，事件发生就是客户端），通过事件收集器来收集和分发事件（调用函数处理），事件收集器：qpoll_wait()函数，ngx_event_accept()，ngx_wait_request_handler()，都属于事件处理器，用来消费事件。

ngx_epoll_process_events就是中间的圆柱

![](E:\jianguoyun\20190413164642739.png)

（6）一道腾讯后台开发的面试题

【问题】使用Linux_epoll模型，水平触发模式；当socket可写时，会不停地触发socket可写的事件，如何处理（就是往缓冲区发满，系统就会告诉你可写可写可写）？

【答案】第一种最普遍的方式：需要向socket写数据的时候才把socket加入epoll（红黑树），等待可写事件。接受到可写事件后，调用write或者send发送数据，当所有数据都写完后，把socket移除epoll。这种方式的缺点是，即使发送很少的数据，也要把socket加入epoll，写完后再移除epoll，由移动的操作代价。

【一种改进的方式】：开始不把socket加入epoll，需要向socket写数据的时候，直接调用write或者send发送数据。如果返回EAGAIN（可能数据多了就返回这个？），把socket加入epoll，在epoll的驱动下写数据，全部数据发送完毕后，再移除epoll。这种方式的优点是：数据不多的时候可以避免epoll的事件处理，提高效率。

## 4、ET、LT深释，服务器涉及、粘包解决

（1）ET、LT模式深入分析及测试

【回顾】

LT：水平触发/低速模式，这个事件没处理完，就会一直触发；

ET：边缘触发/告诉模式，这个事件通知只会出现一次。

普遍认为ET比LT效率高一些，但是ET编程难度比LT大一些；

【测试】

```c++
//ET测试代码
unsigned char buf[10] = {0};
memset(buf,0,sizeof(buf));
do
{
	int n = recv(c->fd, buf, 2, 0);//每次只收两个字节
	if(n == -1 && errno == EAGAIN)
		break;
	else if(n == 0)
		break;
	ngx_log_stderr(0, "OK，收到的字节数为%d, 内容为%s", n, buf);
}while(1);

//LT测试代码
unsigned char buf[10] = {0};
memset(buf, 0 , sizeof(buf));
int n = recv(c->fd, buf, 2, 0);
if(n == 0)
{
	//连接关闭
	ngx_free_connection(c);
	close(c->fd);
	c->fd = -1;
}
ngx_log_stderr(0, "OK, 收到的字节数为%d, 内容为%s", n, buf);
```

客户端发送abcdefg，ET模式，如果每次触发只用recv收2个字节，那么需要触发4次才可以全部收完（注意字符串最后有'\0'，用telnet还可能有回车符\n），LT模式下，只发送一次即可；如果没有数据可接收，则recv会返回-1。

【思考】

为什么ET模式事件只触发一次？事件被扔到双向链表中一次，被epoll_wait取出后就干掉了。

为什么LT模式事件会触发多次呢？事件如果没有处理完，事件会被多次往双向链表中扔，或者就根本没删除，一直在链表中。

所以LT模式的双向链表中表项要比ET的多一些，所以效率会低一些。

如何选择ET还是LT？如果收发数据包有固定格式，建议采用LT，编程简单，清晰，写好了效率不见得低，因为ET反正也要不断地调用recv；如果收发数据包没有固定格式，可以考虑采用ET模式（nginx官方服务器就是没有固定格式的，用ET）。

（2）我们的服务器设计

a）服务器设计原则总述

通用的服务框架：稍加改造甚至不用改造就可以把它直接应用在很多的具体开发工作中，工作重点就可以聚焦在业务逻辑上。      

b）收发包格式问题提出

比如，网络打拳游戏，第一条命令出拳【1abc2】，第二条加血【1def2|30】，命令：1abc21def2|30。

c）TCP粘包、缺包

【TCP粘包问题】

举例：client发送abc，def，hij，三个数据包发出去：

i）客户端粘包现象

比如send("abc");write()也可以，send("def"); send("hij");因为客户端有一个Nagle优化算法（当每个数据包数据很小时把几个包合成一个包发送，因为如果不这么做每个包都要加很多tcp头、ip头、以太网帧头信息，不划算），这三个数据包被Nagle优化算法直接合并一个数据包发送出去，这就属于客户端粘包。

ii）服务器端粘包现象

不管客户端是否粘包，服务器端都存在粘包的问题。

服务器端两次recv之间可能间隔100毫秒，那可能在这100毫秒内，客户端这三个包都到了，这三个包都被保存到了服务器端的针对该TCP连接收数据缓冲中（abcdefhij），此时再次recv一次，就可能拿到了全部"abcdefhij"，这就叫服务器端的粘包。

【再举一例：缺包】

send("abc......")，8000字节；这个可能被操作系统拆成6个包（MTU=1400B）发送出去了，网络可能出现延迟或者阻塞，服务器端第一次recv() = "ab"，recv() = "c...",...,recv() = "...de"，也就是断断续续地掉几十次才能收到所有数据。

d）TCP粘包、缺包解决【面试】

粘包：要解决的就是把这几个包拆出来，一个是一个；解决粘包的方案很多，比如加一些特殊字符abc$def$hij，服务器程序员不能假设收到的数据包都是善意的、合理的，构造畸形数据包abc#def-hij就会让此服务器程序出问题，万万不可！

如何解决拆包问题：给收发的数据包定义一个统一的格式【规则】；c/s都按照这个格式来，就能够解决粘包问题。

**包格式：包头+包体的格式，其中包头是固定长度（10B），在包头中，有一个成员变量会记录整个包（包头+包体）的长度；这样的话，先收包头，从包头中，我知道了整个包的长度，然后用整个包的长度-10个字节 = 包体的长度。我再收"包体的长度"这么多的字节，收满了包体的长度字节数，我就认为，一个完整的数据包（包头+包体）收完。**

![](E:\jianguoyun\20190414123545652.png)

【收包总结】

i）先收固定长度包头10字节；

ii）收满后，根据包头中的内容，计算出包体的长度：整个长度-10

iii）我再收包体长度这么多的数据，收完了，一个包就完整了；

我们就认为收到了一个完整的数据包；从而解决了粘包的问题。

【注意】官方的nginx的代码主要是用来处理web服务器（一种专用的服务器），代码写的很庞杂；不太适合这种固定数据格式（包头+包体）的服务器（通用性强的服务器，可以应用于各种领域，但是也不太适合做web服务器）。

5、通讯代码精粹之收报解包实战

（1）收包分析及包头结果定义

发包：采用包头+包体，其中包头中记录着整个包（包头+包体）的长度。

i）一个包的长度不能超过30000个字节，必须要有最大值，防止恶意数据包使服务器无端等待300亿个字节；

ii）开始定义包头结构：COMM_PKG_HEADER

```c
//包头结构
typedef struct _COMM_PKG_HEADER
{
    unsigned short pkgLen;    //报文总长度【包头+包体】--2字节
                              //2字节可以表示的最大数字为6万多，_PKG_MAX_LENGTH                               //30000 够用了
	                          //包头中记录着整个包【包头+包体】的长度
    unsigned short msgCode;   //消息类型代码--2字节，用于区别每个不同的命令【不                               //同的消息】
    int            crc32;     //CRC32效验--4字节
                              //防止收发数据中出现收到内容和发送内容不一致的情                                 //况，引入这个字段做一个基本的校验
}COMM_PKG_HEADER,*LPCOMM_PKG_HEADER;
```

iii）结构体对齐问题：为了防止出现字节问题，所有再网络上传输的这种结构，必须都采用1字节对齐方式。

```c
//结构体对齐
#pragma pack（1）//对齐方式，1字节对齐【结构之间成员不做任何字节对齐：紧密地排列                   //在一起】
//...
#pragma pack()  //取消指定对齐，恢复缺省对齐    
```

 （2）收包状态宏定义

收包：粘包、缺包；

收包思路：先收包头 -> 根据包头中的内容确定包体长度并收包体，收包状态（类似状态机），定义4种收包的状态：

```c
#define _PKG_HD_INIT				0    //初始状态，准备接收数据包头
#define _PKG_HD_RECVING 	 1	//接收包头钟，包头不完整，继续接收中
#define _PKG_BD_INIT				2	//包头刚好收完，准备接收包体
#define _PKG_BD_RECVING	  3	//接收包体钟，包体不完整，继续接收钟，处理后直接回到	_PKG_HD_INIT状态
```

   （3）收包实战代码

主要的三个用于收包的变量：

```c
char			dataHeadInfo[_DATA_BUFSIZE_]；  //用于保存收到的数据的包头信息
char			*precvbuf；					 //就是所收到数据要保存的位置
unsigned int 	 irecvlen;					   //要收到多少数据由这个变量指定，和precvbuf配套使用
```

聚焦在ngx_wait_request_handler()函数（epoll_wait走下来调用），同时设置好各种收包的状态（根据包头再判断后续再取多少个字节）：

```c
c->curStat     = _PKG_HD_INIT;		//收包状态处于初始状态，准备接收数据包头
c->precvbuf  = c->dataHeadInfo;		//因为要先收包头，所以收数据的位置就是dataHeadInfo
c->irecvlen    = sizeof(COMM_PKG_HEADER);    //这里指定收数据的长度，先要求收包头这么长字节数据
```

要求：客户端连入服务器后，要主动地（客户端有义务）给服务器先发送数据包，服务器要主动收客户端的数据包，服务器按照包头+包体的格式来收包。

引入一个消息头（结构）STRUC_MSG_HEADER，用来记录一些额外信息；服务器收包时除了包头和包体，再额外附加一个消息头 --> 消息头 + 包头 + 包体；**消息头主要用来处理过时包等问题！**

增加一个分配和释放内存类CMemory；

【内存池问题】本项目中不考虑内存池，内存池主要功能是频繁地分配小块内存时可以节省额外内存开销（代价就是代码更复杂），new速度已经很快了，用内存池不方便。

【代码】recvproc调用recv函数，recv返回0表示正常4次挥手断开连接，recv返回-1表示出错，recv返回n表示正常为收到字节数n。再用如下类似代码取包头数据，再取包体数据，因为包体大小是未知的，所以要在ngx_wait_request_handler_proc_p1（包头收完了调用）中分配内存来存包体，并设置接收数据状态为_PKG_BD_INIT，设置接收数据位置和要收的数据大小（包体大小）。同理如果包体也收完了，就调用ngx_wait_request_handler_proc_plast函数去把消息头+包头+包体放入消息队列。

判断是否收完方法：要收的（irecvlen）等于收到的（reco）字节数。

```c
if(c->curStat == _PKG_HD_INIT) //连接建立起来时肯定是这个状态，因为在ngx_get_connection()中已经把curStat成员赋值成_PKG_HD_INIT了
{        
    if(reco == m_iLenPkgHeader)//正好收到完整包头，这里拆解包头
    {   
        ngx_wait_request_handler_proc_p1(c); //那就调用专门针对包头处理完整的函数去处理把。
    }
    else
    {
        //收到的包头不完整--我们不能预料每个包的长度，也不能预料各种拆包/粘包情况，所以收到不完整包头  【也算是缺包】是很可能的；
        c->curStat = _PKG_HD_RECVING;                //接收包头中，包头不完整，继续接收包头中	
        c->precvbuf = c->precvbuf + reco;            //注意收后续包的内存往后走
        c->irecvlen = c->irecvlen - reco;            //要收的内容当然要减少，以确保只收到完整的包头先
    } //end  if(reco == m_iLenPkgHeader)
} 
```

（4）遗留问题处理

new出来的内存记得从消息队列中循环释放！

（5）测试服务器收包避免推诿扯皮

验证ngx_wait_request_handler()函数是否正常工作，写一个客户端程序，为windows下vs2017的mfc程序，非常简陋，只用于演示目的，不具备商业代码质量，不过**SendData函数值得学习（因为如果服务端收缓冲区可能满）**。

```c
int SendData(SOCKET sSocket, char *p_sendbuf, int ibuflen)
{
    int usend = ibuflen; //要发送的数目
    int uwrote = 0;      //已发送的数目
    int tmp_sret;
 
    while (uwrote < usend)
    {
        tmp_sret = send(sSocket, p_sendbuf + uwrote, usend - uwrote, 0);
        if ((tmp_sret == SOCKET_ERROR) || (tmp_sret == 0))
        {
            //有错误发生了
            return SOCKET_ERROR;
        }
        uwrote += tmp_sret;
    }//end while
    return uwrote;

}
```

# 第六章 服务器业务处理框架

## 1、业务逻辑之多线程、线程池实战

（1）多线程的提出

用"线程"来解决客户端发送过来的数据包。

一个进程跑进来之后，缺省就自动启动了一个"主线程"，也就是我们一个worker进程一启动就等于只有一个"主线程"在跑。

多线程处理的必要性：充值服务器通讯，一般需要数秒到数十秒的通讯时间，一个线程因为充值被卡住，还有其他线程可以提供给其他玩家及时的服务。所以，服务器端处理用户需求（用户逻辑/业务）的时候一般都会启动几十甚至上百个线程来处理，以保证用户的需求能够及时处理。

【设计思路】主线程往消息队列中用inMsgRecvQueue()扔完整包（用户需求），那么一堆线程要从这个消息队列中取走这个包，所在必须要用互斥。也就是说，同一时刻只能有一个线程来操作这个队列。

POSIX（可移植操作系统接口）和POSIX线程：定义了一堆我们可以调用的函数，一般是以pthread_开头，比较成熟，比较好用。

（2）线程池实战代码

a）为什么引入线程池

线程池：就是提前创建好一堆线程，并搞一个类来统一管理和调度这一堆线程（这一堆线程我们就叫线程池）。

当来了一个消息的时候，从这一堆线程中找一个空闲的线程去处理这个消息，活干完之后，这个线程里边有一个循环语句，可以循环回来等待新任务，也就是回收再利用。

【线程池存在意义和价值】

i）事先创建好一堆线程，避免动态创建线程来执行任务，提高了程序的稳定性，有效地规避程序运行之中创建线程有可能失败的风险，如果事先创建一般会成功（因为还没怎么占用系统资源），而且即使失败了程序员也能知道为什么失败。

ii）提高运行效率：线程池中的线程，反复循环再利用，不用创建和释放线程（都需要资源和时间）。

iii）容易管理（使编码更清晰简单）

【注意】为了支持pthread多线程库，gcc末尾要增加-lpthread。

讲解了Create()、ThreadFunc()、StopAll()。

（3）线程池的使用

a）线程池的初始化：Create()

b）线程池工作的激发：就是让线程池开始干活了，当收到了一个完整的用户来的消息的时候，就要激发这个线程池获取消息开始工作；用pthread_cond_signal来唤醒一个（或多个）卡在pthread_cond_wait的线程。精华代码如下：

```c
while((jobbuf = g_socket.outMsgRecvQueue()) == NULL && m_shutdown == false)
{
     //如果这个pthread_cond_wait被唤醒【被唤醒后程序执行流程往下走的前提是拿到了锁--官      //方：pthread_cond_wait()返回时，互斥量再次被锁住】，
    //那么会立即再次执行g_socket.outMsgRecvQueue()，如果拿到了一个nullptr，则继续在这
    //里wait着();
    if(pThread->ifrunning == false)            
    	pThread->ifrunning = true; //标记为true了才允许调用StopAll()：测试中发现如
        //果Create()和StopAll()紧挨着调用，就会导致线程混乱，所以每个线程必须执行到
        //这里，才认为是启动成功了；        
    //ngx_log_stderr(0,"执行了pthread_cond_wait-------------begin");
    //刚开始执行pthread_cond_wait()的时候，会卡在这里，而且m_pthreadMutex会被释放掉；
    pthread_cond_wait(&m_pthreadCond, &m_pthreadMutex); //整个服务器程序刚初始化
    //的时候，所有线程必然是卡在这里等待的；
    //ngx_log_stderr(0,"执行了pthread_cond_wait-------------end");
}
```

c）线程池完善和测试

如果只开一个线程，而来多个消息，发现消息会堆积但是不会丢消息，逐条处理，因为每次while消息队列都不为空，所以不会卡在pthread_cond_wait，也就不需要多次call来调用pthread_cond_signal用来唤醒，一次足矣；直到消息队列中没有消息了，它才会再次趴在pthread_cond_wait处等待唤醒。

【注意】pthread_cond_signal可能唤醒多个pthread_cond_wait，惊群（虚假唤醒）现象，但是因为唤醒后，都需要回while去消息队列中取数据，所以如果来了一个消息，唤醒了3个线程，那么一定有一个线程先拿到锁，走下来，可以取到数据，别的线程虽然依次拿到锁走下来，但是因为消息队列中没有数据，所以跳不出while，依旧卡在pthread_cond_wait处。

## 2、业务逻辑之打通业务处理脉搏实战

（1）线程池代码调整及补充说明

支撑线程池的运作主要靠两个函数：

pthread_cond_signal(&**m_pthreadCond**)； =>触发，pthread_cond_wait(&**m_pthreadCond**，&m_pthreadMutex); =>等待

**条件变量**：是线程可用的另一种同步机制，条件变量给多个线程提供了一个会合的场所，条件变量与互斥量一起使用时，允许线程以无竞争的方式等待特定的条件发生。

a）条件本身【while（（pthreadPoolObj ->m_MsgRecvQueue.size() == 0）&& m_shutdown == false）】是由互斥量保护的（也就是执行这一行之前要先锁住互斥量，这样保证了在拿到锁的线程执行下来的时候，别的线程是不会拿到锁执行下来，也就不会执行这条语句，也就察觉不到条件改变（消息队列有变化））。

c++11，也有条件变量的说法 my_cond.wait(...)，my_cond.notify_one(...)。

b）传递给pthread_cond_wait的互斥量m_pthreadMutex是用来对条件【while((pthreadPoolObj->m_MsgRecvQueue.size() == 0) && m_shutdown == false)】进行保护的，调用者把锁住的互斥量传递给函数pthread_cond_wait，函数自动把调用线程放在等待条件的线程列表上，对互斥量解锁（**说白了就是卡在这里等条件直到signal触发，然后放开互斥量**）。

条件变化时，pthread_cond_wait会尝试拿锁，pthread_cond_wait返回成功，互斥量也就被再次锁定，没拿到继续等。

（2）线程池实现具体业务之准备代码

a）一个简单的crc32校验算法介绍

CRC32类：主要目的是对收发的数据包进行一个简单的校验，以确保数据包中的内容没有被篡改过。

Get_CRC()：给一段buffer，也就是一段内存，然后给这段内存长度，该函数计算出一个数字来（CRC32值）返回来。

客户端要把发送的包体计算出来一个CRC32值放入包头对应字段，服务器端收到数据时也要取出包体计算这个CRC32值，然后与收到的包头中对应字段比较，如果相等，则认为该数据包的内容没有被 篡改。

b）引入新的CSocket子类

真正项目中要把CSocket类当成父类使用，具体业务逻辑代码应该放在CSocket的子类中。

threadRecvProcFunc()收到消息之后的处理函数（替换上一节的测试代码）。

c）设计模式题外话

不要乱用设计模式，不要乱封装！

d）消息的具体设计

为了能够根据客户端发送过来的消息代码(msgCode)迅速定位到要执行的函数，我们就把客户端发送过来的消息代码直接当作一个数组的下标来用。

服务器开发工作（业务逻辑），主要集中在三个文件中：ngx_logiccomm.h，ngx_c_slogic.cxx，ngx_c_slogic.h

（3）threadRecvProcFunc()函数讲解

（4）整体测试工作的开展

服务器开发工作，公司配备专门客户端开发人员来开发客户端工作；c/s配合工作，配合指定通讯协议；协议的制定一般是服务器程序员来主导。

a）确定通讯格式是包头+包体，包头固定多少个字节，这种规则在开发一个项目之前，要明确地和客户端交代好；要求客户端给服务器发送数据包时严格遵循这种格式。

b）注册、登录，都属于具体的业务逻辑命令，这种命令一般都是由服务器牵头来制定。

**服务器开发难度往往比客户端大很多，要站在客户端的角度负责通讯协议的制定工作，要学会和客户端商量，共同指定协议（消息代码）和数据结构（双方通信结构）**。

此外，服务器由责任把crc32算法给到客户端（如果用的不是c++还要客户端自己改造）！

## 3、预发包，多线程资源回收深度思考

（1）业务逻辑细节写法说明

_HandleRegister()，_HandleLogin()里边到底执行什么，是属于业务逻辑代码，自己写的。

（2）连接池中连接回收的深度思考

a）一个问题

服务器是一周7天一天24小时不间断工作的，服务器稳定性是第一位的。

【连接池连接回收问题】

如果客户端【张三】断线，服务器立即回收连接，这个连接很可能被紧随其后连入的新客户端【李四】所使用，那么这里就很可能产生麻烦：

i）张三发数据，服务端从线程池中找一个线程调用funca()处理，要执行10秒；

ii）执行到第5秒的时候，张三断线；

iii）张三断线这个事情会被服务器立即感知到（epoll机制），服务器随后调用ngx_close_connection把原来属于张三这个连接池中的连接给回收了；

iv）第7秒的时候，李四连上来了，系统会把刚才张三用过的连接池中的连接分配给李四（现在这个连接归李四使用）；

v）10秒到了，这个线程很可能会继续操纵连接（修改读数据），连接属于李四但是程序还以为是张三的。

【解决办法】一个连接，如果我们程序判断这个连接不用了，那么不应该把这个连接立即放到空闲队列里，而是应该放到一个地方，等待一段时间（60s），60秒之后，再真正的回收这个连接到连接池/空闲队列中去，这种连接才可以真正的分配给其他用户使用。

**为什么要等待60秒？就是需要确保即便用户张三真断线了，那么我执行的该用户的业务逻辑也一定能在这个等待时间内全部完成；这个连接不立即回收是非常重要的，有个时间缓冲，这个可以在极大程度上确保服务器的稳定。**

b）灵活创建连接池

如果按上述所述，连接池满了，我断开一个连接，这个连接不会立即放回空闲队列，这时如果再来一个连接就无法连入了（因为满了，空的那个还用不了），所以最好是动态创建连接池，也就是需要的话就重新new，而不是一上来就new定好的内存。

c）连接池中的连接的回收

i）**立即回收**（accept用户没有接入时可以立即回收）：gx_free_connection()；

ii）**延迟回收**（用户接入进来开始干活了）：inRecyConnectQueue()把要回收的线程放入一个"垃圾队列"，隔80s后调用ServerRecyConnectionThread()函数来释放回收放入空连接队列。

（3）程序退出时线程的安全终止

如何优雅的退出程序（回收资源等）？

（4）epoll事件处理的改造

a）增加新的事件处理函数，引入ngx_epoll_oper_event()函数取代ngx_epoll_add_event()，扩展性好；

b）调整对事件处理函数的调用：改动ngx_epoll_init()，ngx_event_accept()，ngx_epoll_process_events();

**iCurrsequence这个变量可以防止套接字的张冠李戴，因为释放的时候也要加一，只要不等就丢包即可！**

(5）连接延迟回收的具体应用

recvproc()调用了inRecyConnectQuue(延迟回收)取代了ngx_close_connection(立即回收);

Initialize_subproc()子进程中干；Shutdown_subproc()子进程中干：**先让线程停止干活，再清队列，再清信号、互斥量。**

## 4、LT发数据机制深释、gdb调试浅谈

（1）水平触发模式（LT）下发送数据深度解释

在水平触发模式下，发送数据有哪些注意事项？

调用ngx_epoll_oper_event（EPOLL_CTL_MOD, EPOLLOUT）修改红黑树结点，那么当socket可写的时候，会触发socket的可写事件，我得到了这个事件（系统通知我）我就可以发送数据了。

什么叫socket可写：每一个tcp连接（socket），都会有一个接收缓冲区和发送缓冲区，发送缓冲区缺省大小一般10几k，接收缓冲区大概几十k，setsocketopt()来设置；服务端send()，write()发送数据时，实际上这两个函数是把数据放到了发送缓冲区，之后这两个函数返回了；客户端用recv，read()来接收数据。

**如果服务器端的发送缓冲区满了，那么服务器再调用send(),write()发送数据的时候，那么send()、write()函数就会返回一个EAGAIN，EAGAIN不是一个错误，只是示意发送缓冲区已经满了，让程序员迟一些再调用send()、write()来发送数据。**

（2）gdb调试浅谈

a）当socket可写的时候（发送缓冲区没满），会不停地触发socket可写事件（水平触发模式），已经验证。

b）连续发包2次，子进程worker进程突然崩溃，但是不知道是哪出了问题，所以需要借助gdb调试来找到崩溃行。

i）编译时 g++ 要带这个-g选项；

ii）su进入root权限，然后gdb nginx调试

iii）gdb缺省调试主进程，但是gdb7.0以上版本可以调试子进程（需要调试子进程，因为干活的是worker process是子进程）

iv）为了让gdb支持多进程调试，要设置一下follow-fork-mode选项，这是个调试多进程的开关；取值可以是parent[主]/child[子]，这里需要设置成child才能调试worker process子进程；在gdb下输入show follow-fork-mode可以查看follow-fork-mode值，输入set follow-fork-mode child可以变parent为child。

v）还有个选项detach-on-fork，取值为on/off，默认是on（表示只调试父进程或者子进程其中的一个）；调试是父进程还是子进程，由上边的follow-fork-mode选项说了算；如果detach-on-fork = off，就表示父子都可以调试，但是要注意，调试一个进程时，另外一个进程会被暂停，比如ffm是parent，那么fork后的子进程是不运行的。

vi）b logic/ngx_c_slogic.cxx：198 掐断点（b 文件名: 行号）

vii）run 运行程序运行到断点

viii）停在断点了，可以print打印变量值来做一些测试

iX）c继续运行
ev.data.ptr = (void*) pConn;解决问题！（**内核epoll的实现，在用EPOLL_CTL_MOD修改事件的时候，把EPOLL_CTL_MOD对应的struct epoll_event结构也进行了拷贝，所以如果EPOLL_CTL_MOD中struct epoll_event被memset为0了，没有再给ev.data.ptr 进行赋值的话，那么ptr指向为空，后续epoll_wait获取到事件后，取出ev.data.ptr中的操作进行调用的话就是非法操作了**）

（3）前面问题的解决

**【面试】epoll，水平触发模式下，不停地从触发socket可写事件（可读虽然也会不断提醒，但是只要接收缓冲区里的数据都取走处理了，就不会再提醒了，但只要发送缓冲区中有空，就会不停触发可写事件，很麻烦），如何解决？**

两种解决方案：
a）第一种最普遍的解决方案

需要向socket写数据的时候把socket写事件通知加入到epoll中，等待可写事件，当可写事件来时操作系统会通知（epoll_wait走下来）；此时可以调用write/send函数发送数据，当发送数据完毕后，把socket的写事件通知从红黑树中移除；

**缺点**：即使发送很少的数据，也需要把事件通知加入到epoll，写完毕后，又需要把写事件通知从红黑树中干掉，对效率有一定的影响（有一定的操作代价）

b）改进方案（利用EAGAIN）

开始不把socket写事件通知加入到epoll，当我需要写数据的时候，直接调用write/send发送数据；如果返回了EAGAIN（发送缓冲区满了，需要等待可写事件才能继续往缓冲区里写数据），再把写事件通知加入到epoll，此时，就变成了在epoll驱动下写数据，全部数据发送完毕后，再把写事件通知从epoll中干掉。

**优点**：数据不多的时候，可以避免epoll的写事件的增加/删除，提高了程序的执行效率。

## 5、发数据、信号量、并发、多线程综合实战

（1）发送数据指导思想

把要发送的数据放到一个队列中（msgSend），然后专门创建一个线程（ServerSendQueueThread）来统一负责数据发送。

（2）发送数据代码实战

a）信号量：也是一种同步机制，跟互斥量有什么不同呢？

互斥量：线程之间同步；**信号量：提供进程之间的同步，也能提供线程之间的同步。**

【使用方法】

```c
if（sem_init(&m_semEventSendQueue, 0, 0) == -1）{}
//第一个参数：信号量名字
//第二个参数=0：表示信号量在线程之间共享，如果非0，表示在进程之间共享
//第三个参数=0：表示信号量的初始值，为0时，调用sem_wait()就会卡在那么卡着
```

用之前调用sem_init()初始化一下，信号量的初始值设置为0，用完后用sem_destroy()释放信号量。

sem_wait()：测试指定信号的值，如果该值>0，那么将该值-1然后该函数立即返回；如果该值等于0，那么该线程将投入睡眠中，一直到该值>0，这个时候那么将该值-1然后该函数立即返回。

sem_post()：能够将指定信号量值+1，即便当前没有其他线程在等待该信号量值也没关系。

**b）数据发送线程**
**ServerSendQueueThread：按照上节的方法，还处理了多种错误和特殊情况。**

c）可写通知到达后数据的继续发送函数

ngx_write_request_handler()：这里还可能发不完，那就继续靠系统驱动，此外程序最后一定要执行一次sem_post，因为可能有别的线程正卡在sem_wait。

d）发送数据的简单测试

发送缓冲区大概10几k，如何把发送缓冲区撑满？

i）每次服务器给客户端发送65K左右的数据，发送到第20次才出现服务器的发送缓冲区满；这时客户端收了一个包（点击收包），此时执行了ngx_write_request_handler()；

ii）又发包，连续成功发送了16次，才又出现发送缓冲区满；客户端再收包，结果连续收了16次包，服务器才又出现ngx_write_request_handler()函数被成功执行，这表示客户端连续收了16次包，服务器的发送缓冲区才倒腾出地方来；

iii）此后，大概服务器连续发送16次才再出现发送缓冲区满提示，客户端再连续收16次，服务器端ngx_write_request_handler被执行（服务器的发送缓冲区有地方）

![](E:\jianguoyun\20190418180801834.png)

当发送端调用send()发送数据时，操作系统底层已经把数据发送到了该连接的接收端的接收缓存，这个接收缓存大概有几百k。只有当发送的数据量把服务器端的发送缓冲区和客户端的接收缓冲区都填满才可能返回EAGAIN，不要以为几十k就可以填满。

# 第七章 画龙点睛之服务器安全与完善

## 1、过往总结、心跳包代码实战

（1）核心架构浓缩总结实现的功能

a）服务器按照包头包体格式正确的接收客户端发送过来的数据包；

b）根据手动的包的不同来执行不同的业务处理逻辑；

c）把业务处理产生的结果数据包返回客户端。

（2）用到的主要技术

a）epoll高并发通讯技术

b）线程池技术来处理业务逻辑

c）线程之间的同步技术包括互斥量、信号量

其他技术：信号，日志打印，fork()子进程，守护进程

（3）借鉴了哪些官方nginx的精华代码

a）master进程，多个worker子进程----进程框架；

nginx：热更新，worker子进程挂了master能够重启启动worker，重新配置文件等。

b）借鉴了epoll的一些实现代码；官方nginx用的ET【边缘触发模式】，本项目中用的是水平触发模式LT；

c）借鉴了接收数据包，以及发送数据包的核心代码。

（4）哪些内容我们没有借鉴官方nginx呢？

a）比如epoll技术中我们采用LT模式来书写网络数据的接收和发送；

b）自己写一套线程池来处理业务逻辑，调用适当的业务逻辑处理函数，直至处理完毕把数据发送回客户端；

c）连接池中连接的延迟回收，以及专门处理数据发送的发送线程。

（2）心跳包概念【面试】

c/s程序：都有责任把心跳包机制在程序代码中实现好，以确保程序良好的工作以及应付意外的情形发生。

a）什么叫心跳包以及如何使用

心跳包其实就是一个普通的数据包，一般每隔几十秒，最长一般也就是1分钟（10秒-60秒之间），由客户端主动发送给服务器，服务器收到之后，一般会给客户端返回一个心跳包。

**三次握手，tcp连接建立之后，才存在发送心跳包的问题-如果c不给s发心跳包，服务器（约定30秒发送一次）可能会在90秒或者100秒内，主动关闭该客户端的socket连接。**

作为一个好的客户端程序，如果你发送了心跳包给服务器，但是在90或者100秒之内，你【客户端】没有收到服务器回应的心跳包，那么你就应该主动关闭与服务器端的连接，并且如果业务需要重连，客户端程序在关闭这个连接后还要重新主动再次尝试连接服务器端；客户端程序也有必要提示使用者与服务器的连接已经断开。

b）为什么引入心跳包

常规客户端关闭，服务器能感知到；有一种特殊情况，连接断开c/s都感知不到。

为了判断连接后c/s长时间不发送数据是否已经断线，tcp本身提供keepalive机制，但因为检测时间不好控制，不采用。

**比如，c/s程序运行在不同的两个物理电脑上，tcp已经建立，拔掉c/s程序的网线，拔掉网线导致服务器感知不到客户端断开。**

**为了应对拔网线，导致不知道对方是否断开了tcp连接这种事，引入心跳包机制！**

c）心跳包的作用

超时没有发送来心跳包，那么就会将对端的socket连接close掉，回收资源；其他作用为检测网络延迟等等。

这里引入的主要目的就是检测双方的链接是否断开。

（3）心跳包代码实战

a）接收心跳包与返回结果

心跳包（ping包）：规定消息代码为0；一般心跳包也不需要包体，只有包头就够了。

b）处理不发送心跳包的客户端

超过30*3 + 10 = 100秒，仍旧没收到心跳包，那么服务器端就把tcp断开。

## 2、控制连入数，黑客攻击防范及畸形包应对

（1）控制并发连入数量

epoll支持高并发：数万，数十万，百万（一台计算机）；

并发数量取决于很多因素：
a）采用的开发技术：epoll，支持数十万并发

b）程序收发数据的频繁程度，以及具体要处理的业务复杂程度

c）实际的物理内存：可用的物理内存，会直接决定你能支持的并发连接

d）一些其他的tcp/ip配置项

一般，我们日常所写的服务器程序，支持几千甚至1-2万的并发，基本上就差不多了；一个服务器程序，要根据具体的物理内存，以及我们具体要实现的业务等等因素，控制能够同时连入的客户端数量；如果允许客户端无限连入，那么服务器肯定会在未来某时崩溃。

**控制连入用户数量的解决思路：如果同时连入的用户数量超过了允许的最大连入数量时，就把这个连入的用户直接踢出去。**

（2）黑客攻击的防范

【攻击效果】

a）轻则：服务器工作延迟，效率明显降低

b）重则整个服务器完全停摆，没有办法提供正常服务，比如拒绝服务攻击就会导致这种状况（服务器失去响应）；

c）最甚者，如果服务器程序写的有一些漏洞的话，恶意黑客很可能利用给一些远程溢出攻击手段直接攻破你的服务器程序所在的计算机；能拿到一定的权限，甚至可能是root权限，一旦拿到了root权限，那么你的整个计算机都在黑客控制中了。

有些攻击是利用tcp/ip协议先天的一些设计问题来攻击，比如syn flood攻击。

DDOS攻击（syn flood攻击也是DDOS攻击的一种）甚至防火墙等设备都可能防不住，甚至得从数据路由器想办法；

a）、flood攻击防范

以游戏服务器为例，假设我们认为一个合理得客户端一秒钟发送数据包给服务器不超过10个（手点不了那么快），如果客户端不停得给服务器发数据包，1秒钟超过10个数据包，那我服务器就认为这个玩家有恶意攻击服务器得倾向；我们服务器就应该果断地把这个TCP客户端连接关闭，这个也是服务器发现恶意玩家以及保护自身安全得手段。

代码上如何实现1秒钟超过10个数据包则把客户端踢出去；连续不到100ms都收到了数据包10次就干掉。

增加测试函数TestFlood()位置：ngx_read_request_handler()、ngx_wait_request_handler_proc_p1()、ngx_wait_request_handler_proc_plast()。

b）畸形数据包防范

【意识】客户端发送过来的数据并不可信，因为这些数据有可能是造假的，甚至可能是畸形的。

以游戏服务器为例：
i）造假：服务器端书写的时候要细致判断，基本能够避免客户端造假；

ii）**畸形数据包：远程出攻击，攻击成功的主要原因就是服务器程序书写不当，比如接收到的数据缺少边界检查。**

以_HandleRegister()函数为例，我们有必要在字符数组末尾自己增加一个"\0"(字符串结束标记)，以确保我们用这个字符数组时的时候不会出问题：

```c++
p_RecvInfo->username[sizeof(p_RecvInfo->username) - 1] = 0;
p_RecvInfo->password[sizeof(p_RecvInfo->password) - 1] = 0;


```

c）超时直接踢出服务器的需求

账号服务器：账号服务器有一个需求，任何一个连接不超过20秒，超过20秒不主动断开与本服务器连接的，那么本服务器就主动地把这个用户踢下线（断开TCP连接）。

## 3、超负荷安全处理、综合压力测试

（1）输出一些观察信息

每隔10秒钟把一些关键信息显示在屏幕上：
a）当前在线人数；

b）和连接池有关：连接列表大小，空闲连接列表大小，将来释放的连接多少个；

c）当前时间队列大小；

d）收消息队列和发消息队列大小；

打印统计信息的函数：printTDInfo()，每10秒打印一次重要信息。

（2）遗漏的安全问题思考

a）收到太多数据包处理不过来

限速：epoll技术，一个限速的思路；在epoll红黑树节点钟，把这个EPOLLIN（可读）通知干掉。

b）积压太多数据包发送不出去：壮士断腕，丢弃待发送的数据包。

c）连入安全的进一步完善

如果某些恶意用户连上来发了1条数据就断，并不断地产生这种连接，会导致频繁调用ngx_get_connection()使用我们短时间内产生大量连接，而且，这种连接还因为延迟调用的原因在垃圾队列里等待，容易让内存不断增大，以至崩溃。

（3）压力测试：长时间运行程序，最好用多个物理电脑测试。

【测试什么】

a）程序崩溃，这明显不行，肯定要解决；

b）程序运行异常，比如过几个小时，服务器连接不上了，没有回应了，发过来的包服务器处理不了了；

c）服务器程序占用的内存才能不断增加，增加到一定程度，可能导致整个服务器崩溃；

```shell
top -p 3645    //显示进程占用的内存和cpu百分比，用q可以退出
cat /proc/3645/status    //VmRSS: 7700 kB
```

【发现错误】最大连接只在1000多个，且日志中一直报：CSocekt::ngx_event_accept()中accept4()失败

这个跟用户进程可打开的文件数限制有关：因为系统为每个tcp连接都要创建一个socket句柄（accept返回），每个socket句柄同时也是一个文件句柄，比如日志、监听套接字等也是要占用文件句柄的。

```shell
ulimit -n    //1024
```

**必须修改linux对当前用户的进程同时打开的文件数量的限制！**

## 4、惊群、性能优化大局观

（1）cpu占比与惊群

top -p pid，推荐文章：https://www.cnblogs.com/dragonsuc/p/5512797.html

【惊群现象】1个master进程4个worker进程，一个连接进入，惊动了4个worker进程，但是只有一个worker进程accept()，其他三个worker进程被惊动，这就叫惊群；但是，这三个被惊动的worker进程都做了无用功（操作系统本身的缺陷）。

官方nginx解决惊群的办法：锁，进程之间的锁；谁获得这个锁，谁就往监听端口增加EPOLLIN标记，有了这个标记，客户端连入就能够被服务器感知到。

**3.9以上内核版本的linux，在内核中解决了惊群问题；而且性能比官方nginx解决办法效率高很多。**

内核采用reuseport（复用端口），是一种套接字的复用机制，允许将多个套接字bind到同一个ip地址/端口上，这样一来，就可以建立多个服务器来接收到同一个端口的连接（多个worker进程能够监听同一个端口）；

很多套接字配置项可以通过setsockopt()等等函数来配置，还有一些tcp/ip协议的一些配置项我们可以通过修改配置文件来生效。

**惊群问题应该通过操作系统来解决！**

（2）性能优化大局观

从两个方面看下性能优化问题：

【软件层面】

a）充分利用cpu，比如刚才惊群问题；

b）深入了解tcp/ip协议，通过一些协议参数配置来进一步改善性能；

c）处理业务逻辑方面，算法方面有些内容，可以提前做好。

【硬件层面（花钱搞定）】

a）高速网卡，增加网络带宽；

b）专业服务器；数十个核心，马力极强；

c）内存：容量大，访问速度快；

d）主板，总线不断升级的。

（3）性能优化的实施

a）绑定cpu、提升进程优先级

i）一个worker进程运行在一个核上，为什么能够提高性能呢？不存在进程切换问题。

cpu：缓存和cpu缓存命中率问题；把进程固定在cpu核上，可以大大增加cpu缓存命中率，从而提高程序运行效率。

ii）提升进程优先级（setpriority()函数），使进程有机会被分配到更多的cpu时间（时间片），得到执行的机会就会增多。

iii）一个服务器程序，一般只放在一个计算机上跑，专用机。

b）tcp/ip协议的配置选项

这些配置选项都有缺省值，通过修改，在某些场合下，对性能可能会有所提升；

若要修改这些配置项，要做到以下几点：
i）对这个配置项有明确的理解；

ii）对相关的配置项，记录它的缺省值，做出修改；

iii）要反复不断地亲自测试，亲自验证；仔细判断是否提升性能，是否有副作用再使用。

（4）tcp/ip协议的配置选项

a）滑动窗口的概念

为了解决高速传输和流量控制问题（限速问题），滑动窗口的实现一般都会在操作系统内核里边干，但是还是要了解相关的概念。

b）Nagle算法的概念

这个算法是避免发送很小的报文，大家都知道，报文再小它也要有包头，那么我们把几个小报文组合成一个大一点的报文再发送，那至少我们能够少发好几个包头，节省了流量和网络带宽。

c）Cork算法

比Nagle更硬气，完全禁止发送小报文，除非累积到一定数量的数据或者是超过某一个时间才会发送这种报文。

d）Keep-Alive机制

用于关闭已经断开的tcp连接，之前提到过，具体使用方法如下：

net.ipv4.tcp_keepalive_time：探测包发送周期单位是秒，默认是7200（2小时）：如果2小时没有任何报文在这个连接上发送，那么开始启动keep-alive机制，发送keepalive包。

net.ipv4.tcp_keepalive_intvl：缺省值75（单位秒）如果发送keepalive包没应答，则超过75秒再次发送keepalive包；

net.ipv4.tcp_keepalive_probes：缺省值9，如果发送keepalive包对方一直不应答，发送9次后，如果仍然没有回应，则这个连接就被关闭掉。

e）SO_LINGER选项

延迟关闭选项，一般调用setsockopt(SO_LINGER)，这个选项设置在连接关闭的时候，与是否进行正常的四次挥手有关；因为缺省的tcp/ip协议在四次挥手时可能会导致某一通讯方给对方发送rst包以结束连接，从而导致对方收到rst包而丢弃收到的数据，那么**这个延迟选项可以避免给对方发送rst包导致对方丢弃收到的数据**；

说得直白一点：这个选项用来设置延迟关闭的时候，等待套接字发送缓冲区中的数据发送完成。

（5）配置最大允许打开的文件句柄数（遗留问题）

```shell
cat /proc/sys/fs/file-max    //查看操作系统可以使用的最大句柄数
cat /proc/sys/fs/file-nr     //查看当前已经分配的，分配了没使用的，文件句柄最大数目
ulimit -n                    //查看系统允许的当前用户进程打开的文件数限制
ulimit -HSn 5000             //临时设置，只对当前session有效；n:表示我们设置的是文件描述符
```

**如果想要永久限制用户使用的最大句柄数：修改/etc/security/limit.conf文件**

root soft nofile 60000 或在程序中 setrlimit(RLIMIT_NOFILE)

root hard nofile 60000 （soft数字要小于hard，在程序中不可修改）

（6）内存池补充说明

【为什么没有用内存池技术】内存池在多次分配小块内存时好，但是本项目中分配和释放内存都比较快，所以感觉必要性不大；

TCMalloc（谷歌开发的内存分配库）用来取代malloc，虽然快但是稳定性不好说。





